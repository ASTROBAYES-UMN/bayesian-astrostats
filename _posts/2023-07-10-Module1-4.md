---
layout: post
title: "Introductory Probability and Statistics"
date: 2023-07-10
categories: 5731
tags: test demo pleaseWork
pin: false
---

# Introductory Probability and Statistics
_originally made by Galin Jones_

This tutorial was originally made with R and then adapted to also include the same examples in Python.
If you notice any typos or errors please feel free to email me and I will attempt to fix them as soon as I am able.

Some R packages that will be used to construct parts of these notes.
```
library(tidyverse)
library(ggplot2)
library(modelsummary)
```
## Scope

These notes constitute the briefest introduction to the essentials of probability and statistical
theory useful for our course. Indeed the material presented here is foundational for everything
we will do later, but it is also standard. As such there are already _many_ online resources for
each section.

Most courses on mathematical statistics (e.g. Mathematical Statistics STAT 4101-2, STAT
5101-2, STAT 8101-2) cover probability and statistical theory in vastly more depth and breadth.
MATH 8651-2 (Theory of Probability Including Measure Theory) provides a rigorous course
on probability while MATH 5651-2 provides a more operational treatment.

Introductions to Bayesian methods often spend a fair amount of time on two, of many possible
interpretations of probability; subjectivist versus frequentist. That is not part of the current
program. The interested reader is directed to the Stanford Encyclopedia of Philosophy(Hà-
jek 2020) for a fuller discussion of the interpretations of probability. Cox (1946) provides a
beautiful classic paper on this topic from a physics point of view.

## Experiments, sample spaces, and events

Definition 0.1. ---
: An _experiment_ is an action or process that generates outcomes. An experi-
ment produces only one outcome.

A few examples.

- Flip a coin once. Observe H or T. (Bernoulli trial)
- Flip the same coin 10 times, independently. Observe the number of H. (Binomial experiment)
- Conduct an election. Observe proportion of votes for each candidate.
- Randomized clinical trial to test a vaccine. Observe how many contract the virus and any potential side effects.
- Citizen science via Zooniverse–Hubble Asteroid Hunter project.
- Examine GPA by major for past students.
- Search to find a galaxy and a quasar within a specified angular distance of each other.
- And so on.

Definition 0.2. ---
: The _sample space_ , denoted𝒳, is the set of all possible outcomes.

Some examples.

- Flip a coin once $\mathcal{X}$ = {𝐻, 𝑇 }
- Flip a coin twice {% raw %}$\mathcal{X}${% endraw %} = {𝐻𝐻, 𝐻𝑇 , 𝑇 𝐻, 𝑇 𝑇 }
- Conduct an election. Observe proportion of votes for candidate A. $\mathcal{X}$ = [0, 1]

**Exercise 0.1.** What is the sample space if we flip a coin until the first H? Is it finite?

Definition 0.3. ---
: An _event_ is a set of possible outcomes, that is, any subset of the sample space
and ℱ will denote the space of events.

Notice that the empty set∅and𝒳are included in, but∅cannot occur while𝒳is certain
to occur.

If|, thenℱis the power set of𝒳. More generally, it is a𝜎-algebra, but if you are
unfamiliar with measure theory, ignore this.

## Kolmogorov axioms

A real-valued function,𝑃, on(𝒳, ℱ)is a _probability function_ (pf) if it satisfies the following
axioms.

1. If𝐴is an event, then0 ≤ 𝑃 (𝐴) ≤ 1;
2. 𝑃 (𝒳) = 1; and
3. Any countable sequence of mutually disjoint events𝐴 1 , 𝐴 2 , ...satisfies

#### 𝑃 (∪∞𝑖=1𝐴𝑖) =

```
∞
∑
𝑖=
```
#### 𝑃 (𝐴𝑖).


```
Theorem 0.1. Let 𝑃 be a probability function and let 𝐴 , 𝐴 1 , ... , 𝐴𝑛 and 𝐵 be events. Then
```
_1._ 𝑃 (∅) = 0 _;_
    _2. For any_ 𝑛 ≥ 2 _, if_ 𝐴 1 , ... , 𝐴𝑛 _are mutually disjoint, then_

#### 𝑃 (∪𝑛𝑖=1𝐴𝑖) =

```
𝑛
∑
𝑖=
```
#### 𝑃 (𝐴𝑖);

_3. if_ 𝐴 ⊆ 𝐵 _, then_ 𝑃 (𝐴) ≤ 𝑃 (𝐵) _;
4._ 𝑃 (𝐴𝑐) = 1 − 𝑃 (𝐴) _; and
5._ 𝑃 (𝐴 ∪ 𝐵) = 𝑃 (𝐴) + 𝑃 (𝐵) − 𝑃 (𝐴 ∩ 𝐵)_._

```
Exercise 0.2. Prove the theorem using the axioms. Venn diagrams are helpful.
```
## Independence and conditional probability

```
Conditional probability will be foundational to the remainder of the course. Many also find it
counterintuitive at first so it will pay dividends to spend some time on it. The basic idea is that
we want to find the probability of an event𝐴conditional on an event𝐵having occurred. We
will often say that we want the probability of an event𝐴given𝐵, or𝑃 (𝐴|𝐵), as shorthand.
Let’s begin with an example that may have prompted more arguments than any other because
many find it counterintuitive.
```
```
Example 0.1. Parade magazine’s Ask Marilyn provides a description of the classic Monty
Hall problem: Suppose you’re on a game show, and you’re given the choice of three doors:
Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host,
who knows what’s behind the doors, opens another door, say No. 3, which has a goat. He then
says to you, “Do you want to pick door No. 2?” Is it to your advantage to switch your choice?
Before reading any further do, you think it is advantageous to switch? Does your intuition
suggest that it shouldn’t matter?
Suppose you choose Door 2 (Which door you start with doesn’t matter). What is the proba-
bility of winning if you switch? If you don’t switch? From Table 1 we see that there are three
possibilities and you win only one of them if don’t switch while you win two of them if you
switch. Therefore, the probability of winning is doubled if you switch!
```

```
Table 1:Monte Hall Problem
```
```
Door 1 Door 2 Door 3 No Switch Switch
Goat Car Goat W L
Goat Goat Car L W
Car Goat Goat L W
```
**Example 0.2.** Suppose a student wants to apply to a university that admits 75% of applicants
and of those admitted 15% are awarded a scholarship.

If𝐴denotes the event of being admitted and𝐵denotes the event of being awarded a scholar-
ship, then, because a scholarship is awarded only after admission

```
𝑃 (Awarded scholarship) = 𝑃 (𝐵given𝐴) = 𝑃 (𝐵 ∣ 𝐴) = 0.15.
```
What is the probability that the student is admitted and awarded a scholarship? Well, 15%
of the 75% of those admitted are awarded scholarships so

```
𝑃 (admission and scholarship) = 𝑃 (𝐴 ∩ 𝐵) = 0.15 × 0.75 = 0.1125.
```
**Definition 0.4.** Let𝐴and𝐵be events and assume𝑃 (𝐵) > 0. The _conditional probability_ of
𝐴given𝐵is

#### 𝑃 (𝐴 ∣ 𝐵) =

#### 𝑃 (𝐴 ∩ 𝐵)

#### 𝑃 (𝐵)

#### .

Rearranging we obtain𝑃 (𝐴|𝐵)𝑃 (𝐵) = 𝑃 (𝐴 ∩ 𝐵). Note that if𝑃 (𝐴) > 0and𝑃 (𝐵) > 0,
then

#### 𝑃 (𝐵|𝐴) =

#### 𝑃 (𝐴 ∩ 𝐵)

#### 𝑃 (𝐴)

and by rearranging we obtain𝑃 (𝐵|𝐴)𝑃 (𝐴) = 𝑃 (𝐴 ∩ 𝐵). Hence

#### 𝑃 (𝐵 ∣ 𝐴)𝑃 (𝐴) = 𝑃 (𝐴 ∣ 𝐵)𝑃 (𝐵).

Rearranging this we obtain _Bayes formula_.


**Theorem 0.2.** _If_ 𝑃 (𝐴) > 0 _and_ 𝑃 (𝐵) > 0 _, then_

#### 𝑃 (𝐵 ∣ 𝐴) =

#### 𝑃 (𝐴 ∣ 𝐵)𝑃 (𝐵)

#### 𝑃 (𝐴)

#### .

**Exercise 0.3.** Prove the law of total probability

#### 𝑃 (𝐴) = 𝑃 (𝐴 ∩ 𝐵) + 𝑃 (𝐴 ∩ 𝐵𝑐)

and obtain an alternative form for Bayes formula

#### 𝑃 (𝐵 ∣ 𝐴) =

#### 𝑃 (𝐴 ∣ 𝐵)𝑃 (𝐵)

#### 𝑃 (𝐴 ∣ 𝐵)𝑃 (𝐵) + 𝑃 (𝐴 ∣ 𝐵𝑐)𝑃 (𝐵𝑐)

#### .

**Example 0.3.** In the 2020 presidential election there were 276,765 voters in Wyoming and
70% voted for the Republican candidate while in Minnesota there were 3,277,171 voters and
46% voted for the Republican candidate. If a voter is randomly selected from the 3,553,
total voters of the two states, what is the probability that the voter lives in Wyoming if they
voted for the Republican candidate?

Let𝑊be the event that the voter lives in Wyoming and𝑅the event that they voted for the
Republican candidate. Then we want to find𝑃 (𝑊 |𝑅). Notice that here𝑊𝑐is the event that
the voter lives in Minnesota and that

#### 𝑃 (𝑊 ) =

#### 276, 765

#### 3, 553, 936

#### = 0.078 = 1 − 𝑃 (𝑊𝑐).

Using Bayes rule we have

#### 𝑃 (𝑊 ∣ 𝑅) =

#### 𝑃 (𝑅 ∣ 𝑊 )𝑃 (𝑊 )

#### 𝑃 (𝑅 ∣ 𝑊 )𝑃 (𝑊 ) + 𝑃 (𝑅 ∣ 𝑊𝑐)𝑃 (𝑊𝑐)

#### =

#### 0.7 × 0.

#### 0.7 × 0.078 + 0.46 × 0.

#### = 0.

**Definition 0.5.** The events𝐴and𝐵are _independent_ if and only if

#### 𝑃 (𝐴 ∩ 𝐵) = 𝑃 (𝐴)𝑃 (𝐵).


An alternative and equivalent formulation is that events𝐴and𝐵such that𝑃 (𝐴) > 0and
𝑃 (𝐵) > 0are independent if and only if𝑃 (𝐴|𝐵) = 𝑃 (𝐴).

Suppose𝐴and𝐵are independent. Then

#### 𝑃 (𝐴 ∣ 𝐵) =

#### 𝑃 (𝐴 ∩ 𝐵)

#### 𝑃 (𝐵)

#### =

#### 𝑃 (𝐴)𝑃 (𝐵)

#### 𝑃 (𝐵)

#### = 𝑃 (𝐴).

Suppose𝑃 (𝐴|𝐵) = 𝑃 (𝐴). Then

#### 𝑃 (𝐴) = 𝑃 (𝐴 ∣ 𝐵) =

#### 𝑃 (𝐴 ∩ 𝐵)

#### 𝑃 (𝐵)

and rearranging we obtain𝑃 (𝐴 ∩ 𝐵) = 𝑃 (𝐴)𝑃 (𝐵). Hence𝐴and𝐵are independent.

**Exercise 0.4.** Are mutually exclusive events independent?

**Exercise 0.5.** The following statements are equivalent: 1. 𝐴and𝐵are independent. 2. 𝐴
and𝐵𝑐are independent. 3.𝐴𝑐and𝐵are independent. 4.𝐴𝑐and𝐵𝑐are independent.

## Random variables.

Many (not all!) experiments are conducted so that the outcome generated is a numerical
measurement. It might seem obvious that before an experiment is conducted the outcome is
uncertain and that afterwards there is no uncertainty, but this is a profound distinction.

**Definition 0.6.** A _random variable_ (rv) is a numerical measurement of the outcome of an
experiment that has yet to be performed. More formally, a random variable is a real-valued
function on the sample space.

**Definition 0.7.** A _realization_ of a random variable is the value that the random variable takes
on after an experiment is performed. These are the data.

Upper case letters are reserved for random variables. So𝑋,𝑌,𝑍commonly denote random
variables. Lower case letters denote realizations, so𝑥,𝑦,𝑧.


```
Unique (#) Missing (%) Mean SD Min Median Max
```
```
obsx 43 0 0.0 0. 2 − 0. 5 0 .1 0. 3
errx 28 0 0.0 0.0 0. 0 0 .0 0. 1
obsy 41 0 8.4 0.9 6. 5 8 .6 10. 3
erry 25 0 0.1 0.1 0. 0 0 .1 0. 4
```
**Example 0.4.** A supernova is a star that suddenly increases greatly in brightness because of
an explosion that ejects most of its mass. Consider the event of observing a supernova without
assistance over the previous 10 centuries. The random variable of interest is

```
𝑋 =Number of naked-eye supernova observed in previous ten centuries.
```
Notice that𝑋 ∈ {0, 1, 2, ...}.

According to Wall and Jenkins (2012), in the 10 centuries before 1987, four naked-eye super-
novae were recorded so the realized value of the random variable is𝑥 = 4.

**Example 0.5.** Suppose we are interested in the relationship between the masses of supermas-
sive black holes that lie at the center of a galaxy and the velocity dispersion of the stars in its
bulge. Let𝑀⊙be the solar mass and𝑀•the mass of the black hole. The natural logarithm^1 of
the black hole mass is taken to be𝑌 =log(𝑀•/𝑀⊙)and the natural logarithm of the velocity
dispersion of the stars in its bulge is𝑋 =log(𝜎𝑒/𝜎 0 )with𝜎 0 a known reference value. Then
both𝑋and𝑌 are random variables. The pair(𝑋, 𝑌 )is often called a random vector (a vector
of random variables).

Hilbe, de Souza, and Ishida (2017) provide the data summarized below and presented fully
in Figure 1 which consists of 46 observations of the mass of supermassive black holes at the
center of a galaxy and the velocity dispersion of the stars in its bulge. These are the realized
values(𝑥, 𝑦)of the random vectors(𝑋, 𝑌 ), one for each galaxy.

```
datasummary_skim(bh_dat)
```
```
bh_fig <- ggplot(bh_dat, aes(x=obsx, y=obsy)) + geom_point(size= 1 )+
geom_rug() +
labs(x="Log_10 velocity dispersion",y = "Log_10 black hole mass")
```

```
7
```
```
8
```
```
9
```
```
10
```
```
−0.50 −0.25 0.00 0.
Log_10 velocity dispersion
```
```
Log_10 black hole mass
```
```
Figure 1:Plot of log(𝑀•/𝑀⊙)versus log(𝜎𝑒/𝜎 0 ).
```
We will consider two types of random variables, discrete and continuous. Discrete rvs take
on at most countable many values, while continuous rvs take on uncountably many values.
The random variable defined in Example0.4is discrete while the random variables defined in
Example0.5are continuous.

## Probability distributions.

**Definition 0.8.** A _probability distribution_ consists of a random variable’s possible values and
its probability function.

The remainder of this section describes several probability distributions that we will encounter
throughout the course. This section is not intended to be exhaustive, other important distri-
butions will be described or derived later.

(^1) In statistics the logarithm is taken to mean natural logarithm by default. I will try to be explicit about this,
but if the base is unspecified, then reader should use the default.


### Discrete distributions.

The probability function for a discrete random variable is called a _probability mass function_
(pmf). A pmf is a function𝑝 ∶ 𝒳 → ℝsuch that

#### 𝑝(𝑥) = 𝑃 (𝑋 = 𝑥)

```
so that𝑝(𝑥) ≥ 0for all𝑥 ∈ 𝒳, and
```
#### ∑

```
𝑥∈𝒳
```
#### 𝑝(𝑥) = 1.

```
For a discrete random variable, if𝐴 ∈ ℱ, then
```
#### 𝑃 (𝑋 ∈ 𝐴) = ∑

```
𝑥∈𝐴
```
#### 𝑝(𝑥).

```
Bernoulli distribution
Recall Example 0.4 where we considered the event of observing naked-eye supernova
over the previous 10 centuries. Let’s consider a simpler setting for now. Define
𝑋 = 𝐼 (at least one supernova observed). Then 𝑋 ∈ {0, 1}so that if 𝜃 ∈ (0, 1) denotes
the probability of observing at least one supernova, then, by Kolmogorov’s axioms,
```
#### 𝑃 (𝑋 = 1) = 𝜃

```
and
```
#### 𝑃 (𝑋 = 0) = 1 − 𝑃 (𝑋 = 1) = 1 − 𝜃.

```
We can write this more concisely as
```
#### 𝑃 (𝑋 = 𝑥) = 𝜃𝑥(1 − 𝜃)1−𝑥 𝑥 ∈ {0, 1},

```
which is the pmf corresponding to the Bernoulli distribution. This is a useful distribution for
binary random variables. We write𝑋|𝜃 ∼Bernoulli(𝜃)or just𝑋 ∼Bernoulli(𝜃).
The Bernoulli distribution is a building block for several other common discrete distributions.
We will consider two here–the Discrete Uniform and the Binomial. We begin with the Discrete
Uniform.
```

**Example 0.6.** Suppose𝑋 ∼Bernoulli(1/2). Then

#### 𝑃 (𝑋 = 1) = 𝑃 (𝑋 = 0) = 1/2.

A natural extension is to allow𝑋 = 1, 2, ... , 𝑁and set

#### 𝑃 (𝑋 = 𝑥) = 1/𝑁 𝑥 ∈ {1, 2, ... , 𝑁 }.

This is the _Discrete Uniform_ distribution and we write𝑋 ∼Discrete Uniform(𝑁 ).

**Binomial distribution**

**Example 0.7.** Recall Example0.4where we considered the event of observing naked-eye
supernova over the previous 10 centuries. Let’s make the additional assumptions that observing
at least one supernova in each century is independent of which century is chosen and that the
probability of observing at least one supernova in each century,𝜃, is constant across centuries.
Then we can define random variables𝑌𝑖 = 𝐼 (at least one supernova observed in century𝑖)
for𝑖 = 1, ... , 10. In this case, the random variables𝑌𝑖are independent Bernoulli(𝜃) random
variables and we can define the total number of centuries in which supernovae were observed
as

#### 𝑋 =

```
10
∑
𝑖=
```
#### 𝑌𝑖.

What is the probability distribution of𝑋? We need to derive the probability function. Notice
that𝑋 ∈ {0, 1, 2, ... , 10}and that the event that𝑋 = 4, say, can occur in many ways. For
example, it could be the case that

The number of ways of observing 4 distinct centuries with supernovae out of 10 is

#### {𝑌 1 , ... , 𝑌 10 } = {1, 1, 1, 1, 0, 0, 0, 0, 0, 0}

or

#### {𝑌 1 , ... , 𝑌 10 } = {1, 0, 1, 1, 1, 0, 0, 0, 0, 0}.

Both of these arrangements have probability (by independence)

#### 𝜃^4 (1 − 𝜃)^6.


Indeed, by using independence, no matter which order the four centuries with supernovae are
observed this will be the probability. Thus we can find the probability that𝑋 = 4by finding
the number of ways we can observe supernovae in 4 centuries multiplied by probability of any
one way we observe supernovae in 4 centuries or

```
𝑃 (𝑋 = 4) = [number of ways we can observe supernovae in 4 centuries] × 𝜃^4 (1 − 𝜃)^6
```
The number of ways of observing 4 distinct centuries with supernovae out of 10 is

#### (

#### 10

#### 4

#### ) =

#### 10!

#### 4!6!

#### = 210.

Therefore,

#### 𝑃 (𝑋 = 4) = 210𝜃^4 (1 − 𝜃)^6.

Now the same argument could be made for any of 0 ≤ 𝑋 ≤ 10so we arrive at, for𝑥 ∈
{0, 1, 2, ... , 10},

#### 𝑃 (𝑋 = 𝑥) = (

#### 10

#### 𝑥

#### )𝜃𝑥(1 − 𝜃)10−𝑥.

**Definition 0.9.** Let𝑋be the sum of𝑛independent Bernoulli(𝜃) trials, then the probability
function of𝑋, for𝑥 ∈ {0, 1, ... , 𝑛}and0 ≤ 𝜃 ≤ 1, is

#### 𝑃 (𝑋 = 𝑥) = (

#### 𝑛

#### 𝑥

#### )𝜃𝑥(1 − 𝜃)𝑛−𝑥.

In this case𝑋is a _Binomial_ random variable and we write𝑋|𝜃, 𝑛 ∼Binomial(𝑛, 𝜃)or just
𝑋 ∼Binomial(𝑛, 𝜃).

**Example 0.8.** Suppose𝑋 ∼Binomial(10, .3). When𝑛is small Binomial probabilities can be
computed directly, but it is easy to use R.

```
# P(X=3)
dbinom( 3 , 10 ,. 3 )
```
#### [1] 0.


```
Unique (#) Missing (%) Mean SD Min Median Max
```
```
N_GC 227 0 1506.1 3949.1 0.0 71.5 32500. 0
MV_T 333 0 − 19 .1 2. 8 − 24. 2 − 19. 6 − 11. 2
```
```
# P(X \le 2)
dbinom( 0 , 10 ,. 3 ) + dbinom( 1 , 10 ,. 3 )+dbinom( 2 , 10 ,. 3 )
```
#### [1] 0.

```
#P(X \le 2)
```
```
pbinom( 2 , 10 ,. 3 )
```
#### [1] 0.

**Poisson distribution**

A common setting is to count something which has no natural upper bound, although very
large or very small counts may be unlikely. None of the distributions considered so far are
appropriate for this type of data.

**Example 0.9.** Hilbe, de Souza, and Ishida (2017) provide data of the number of globular
clusters in 420 galaxies. The full data set is plotted in Figure 2 while the galaxies with no
more than 100 globular clusters are plotted in Figure 3.

```
datasummary_skim(NGC_dat)
```
```
NGC_hist<- ggplot(data = NGC_dat,aes(x = N_GC)) +
geom_histogram() +
labs(x = "Number of globular clusters",
y = "Frequency")
```
```
NGC_dat100 <- NGC_dat %>% filter(N_GC<= 100 )
```

```
0
```
```
100
```
```
200
```
```
300
```
```
0 10000 20000 30000
Number of globular clusters
```
```
Frequency
```
```
Figure 2:Frequency plot of globular clusters for all galaxies observed.
```
```
NGC_hist100<- ggplot(data = NGC_dat100,aes(x = N_GC)) +
geom_histogram() +
labs(x = "Number of globular clusters",
y = "Frequency")
```
The Poisson is used a starting point for modelling the distributions of counts. For example, it
can be useful when counting the number of events per unit time or even per unit area. Poisson
processes have even been used to study the nature of coincidences.

**Definition 0.10.** Let𝑋 ∈ {0, 1, 2, ... , }have pmf

#### 𝑃 (𝑋 = 𝑥) =

#### 𝑒−𝜃𝜃𝑥

#### 𝑥!

#### .

Then𝑋|𝜃 ∼Poisson(𝜃).

The Poisson distribution can be informally derived as a limit from the Binomial or more
formally from the so-called Poisson postulates, but this is beyond our scope.


```
0
```
```
10
```
```
20
```
```
30
```
```
40
```
```
0 25 50 75 100
Number of globular clusters
```
```
Frequency
```
```
Figure 3:Frequency plot of globular clusters for subset of galaxies with no more than 100.
```
### Continuous Distributions

**Definition 0.11.** The probability function for a continuous random variable is called a _prob-
ability density function_ (pdf). A pdf is a function𝑝 ∶ 𝒳 → ℝsuch that𝑝(𝑥) ≥ 0for all𝑥 ∈ 𝒳
and

#### ∫

```
𝒳
```
#### 𝑝(𝑥)𝑑𝑥 = 1.

For a continuous random variable, if𝐴 ∈ ℱ, then

#### 𝑃 (𝑋 ∈ 𝐴) = ∫

```
𝐴
```
#### 𝑝(𝑥)𝑑𝑥.

Notice that for continuous random variables𝑃 (𝑋 = 𝑐) = 0for any𝑐while it may be the case
that𝑝(𝑐) > 0(points have no width and so do not contribute to the integral).


**Uniform distribution**

Some continuous distributions have discrete analogues. The discrete uniform is characterized
by having a constant probability for each point. Now consider a continuous random variable,
𝑋, on[0, 1]and suppose the probability of the event0 ≤ 𝑎 ≤ 𝑋 ≤ 𝑏 ≤ 1is constant for
any interval of length 𝑏 − 𝑎. This implies the probability is constant on[0, 1], and, since
𝑃 (0 ≤ 𝑋 ≤ 1) = 1, we have that the probability function is𝑝(𝑥) = 𝐼 (0 ≤ 𝑥 ≤ 1).

**Definition 0.12.** Let𝑋be a continuous random variable on[0, 1]with pdf𝑝(𝑥) = 𝐼 (0 ≤ 𝑥 ≤
1), then𝑋is distributed as the _standard Uniform distribution_ and we write𝑋 ∼Uniform(0, 1).

**Exercise 0.6.** How would the Uniform distribution change if the random variable may take on
values between two arbitrary, but finite, real numbers𝛼 < 𝛽? That is, what is the probability
function if𝑋 ∼Uniform(𝛼, 𝛽)?

**Beta distribution**

Suppose𝒳 = [0, 1], but we want to allow more general shapes to the density function than is
possible with the Uniform distribution. For example, it might be more likely to observe the
event0.45 ≤ 𝑋 ≤ 0.55, than𝑋 ≥ 0.9, say.

**Example 0.10.** Hilbe, de Souza, and Ishida (2017) provide data for the baryon fraction in
atomic gas for 1175 low-mass galaxies. The baryon fraction takes on values between 0 and 1,
but does not appear to be uniformly distributed on[0, 1]; see Figure 4 for an estimated pdf.

```
bary_dat<- bary_dat %>% mutate(fgas = M_HI /(M_HI+ M_STAR))
```
```
bary_dens<- ggplot(data = bary_dat, aes(x = fgas)) +
geom_density() +
labs(x = "Baryon fraction in atomic gas")
```
The Beta(𝛼, 𝛽)distribution, which generalizes the standard Uniform, may be a useful model
for this sort of data.


```
0.
```
```
0.
```
```
1.
```
```
1.
```
```
0.00 0.25 0.50 0.75 1.
Baryon fraction in atomic gas
```
```
density
```
```
Figure 4:Estimated density function of baryon fraction.
```
**Definition 0.13.** Let

#### Γ(𝑧) = ∫

```
∞
```
```
0
```
#### 𝑥𝑧−1𝑒−𝑥𝑑𝑥

and

#### 𝐵(𝛼, 𝛽) =

#### Γ(𝛼)Γ(𝛽)

#### Γ(𝛼 + 𝛽)

#### .

Let𝛼 > 0, 𝛽 > 0. If𝑋is a continuous random variable on𝒳 = [0, 1]with pdf

#### 𝑝(𝑥|𝛼, 𝛽) =

#### 1

#### 𝐵(𝛼, 𝛽)

#### 𝑥𝛼−1(1 − 𝑥)𝛽−1.

then𝑋|𝛼, 𝛽 ∼Beta(𝛼, 𝛽)or just𝑋 ∼Beta(𝛼, 𝛽).

Notice that𝑋 ∼Beta(1, 1)is equivalent to𝑋 ∼Uniform(0, 1).

```
The gamma function
```

#### Γ(𝑧) = ∫

```
∞
```
```
0
```
#### 𝑥𝑧−1𝑒−𝑥𝑑𝑥

```
satisfiesΓ(𝑎 + 1) = 𝑎Γ(𝑎)for𝑎 > 0and, in particular, if𝑛is a positive integer, then
Γ(𝑛) = (𝑛 − 1)!.
```
**Example 0.11.** Figure 5 displays four Beta densities based on different values for the shape
parameters,𝛼and𝛽.

```
0.
```
```
0.
```
```
1.
```
```
1.
```
```
1.
```
```
0.00 0.25 0.50 0.75 1.
shape1=1, shape2=
```
```
y
```
```
0.
```
```
0.
```
```
1.
```
```
1.
```
```
0.00 0.25 0.50 0.75 1.
shape1=1.5, shape2=
```
```
y
```
```
0.
```
```
0.
```
```
1.
```
```
1.
```
```
0.00 0.25 0.50 0.75 1.
shape1=1.5, shape2=
```
```
y
```
```
1
```
```
2
```
```
3
```
```
4
```
```
5
```
```
0.00 0.25 0.50 0.75 1.
shape1=/2, shape2=1/
```
```
y
```
```
Figure 5:Four Beta densities.
```
**Exercise 0.7.** Consider Example0.10. The data can be found at thislink. Read the data
in and construct thefgasfraction. Plot the four beta densities from Example0.11over a
histogram of the constructed fraction. Which most closely agrees with the data?


**Example 0.12.** Beta probabilities can be diﬀicult to calculate analytically but we can useR.
Suppose𝑋 ∼Beta(1.3, 4.1). Then

```
# P(X \ge 0.5)
1 - pbeta(0.5, 1.3,4.1)
```
#### [1] 0.

```
# P(.25 < X \le .5)
pbeta(. 5 , 1.3, 4.1)- pbeta(. 25 , 1.3,4.1)
```
#### [1] 0.

**Gamma distribution**

Just as the Beta distribution allows a variety of shapes on [0, 1], the Gamma distribution
allows a variety on𝒳 = [0, ∞). We will most commonly encounter the Gamma distribution
as a prior on parameters that must be positive.

**Definition 0.14.** Let𝛼 > 0, 𝛽 > 0. If𝑋be a continuous random variable on𝒳 = [0, ∞)with
pdf

#### 𝑝(𝑥|𝛼, 𝛽) =

#### 𝛽𝛼

#### Γ(𝛼)

#### 𝑥𝛼−1𝑒−𝛽𝑥,

then we say𝑋|𝛼, 𝛽 ∼Gamma(𝛼, 𝛽). We call𝛼the shape parameter and𝛽the rate parameter.

The _Exponential distribution_ is a special case of the Gamma distribution where𝛼 = 1. If
𝑋 ∼Gamma(1, 𝛽), then

#### 𝑝(𝑥|1, 𝛽) = 𝛽𝑒−𝛽𝑥.

In this case, we say𝑋|𝛽 ∼Exponential(𝛽).

The _chi-squared_ distribution, or𝜒^2 -distribution, results by setting𝛼 = 𝑘/2, with𝑘a positive
integer, and𝛽 = 1/2. We say𝑋 ∼ 𝜒^2 𝑘and𝑘is called the _degrees of freedom_. The pdf is

#### 𝑝(𝑥|𝑘/2, 1/2) =

#### 1

#### 2 𝑘/2Γ(𝑘/2)

#### 𝑥𝑘/2−1𝑒𝑥/2.


```
Another parameterization of the Gamma density is common
```
#### 𝑝(𝑥|𝛼, 𝛽) =

#### 1

#### Γ(𝛼)𝛽𝛼

#### 𝑥𝛼−1𝑒−𝑥/𝛽.

```
In this parameterization𝛼is still called the shape parameter, but𝛽is called the scale
parameter.
The reader should expect either parameterization of the Gamma distribution in software
packages. Which one is used can and will impact the results.
```
**Exercise 0.8.** Plot the Gamma density with𝛼 = 𝛽 = 0.001,𝛼 = 𝛽 = 1,𝛼 = 2, 𝛽 = 7.5, and
𝛼 = 7.5, 𝑏𝑒𝑡𝑎 = 2.

**Exercise 0.9.** The Poisson and Gamma distributions share many interesting connections.
Suppose𝑋 ∼Gamma(𝛼, 𝛽)with𝛼an integer. Then, for any𝑥, if𝑌 ∼Poisson(𝑥/𝛽),

#### 𝑃 (𝑋 ≤ 𝑥) = 𝑃 (𝑌 ≥ 𝛼).

Hint: Integration by parts is your friend.

## Statistics

**Definition 0.15.** A collection of random variables𝑋 1 , 𝑋 2 , ... , 𝑋𝑛is a _sample_. If the each𝑋𝑖
has the same probability distribution and are independent, then we say that is is a _random
sample_ or the sample is _independent and identically distributed_ (iid).

```
The data ,𝑥 1 , ... , 𝑥𝑛 are realizations of a sample𝑋 1 , ... , 𝑋𝑛, but these terms are often
used interchangeably. It is important to keep the distinction in mind.
```
In introductory statistics courses, the sample is often assumed to be iid, mostly for analytical
convenience, and this will indeed be our starting point. However, we will encounter many
examples later in the course where this is an unrealistic assumption and our job will be to
account for the departure.


**Definition 0.16.** A _statistic_ is a real-valued function of the sample. A statistic is itself a
random variable and its probability distribution is called a _sampling distribution_.

Three statistics that we will encounter often are the _sample mean_

#### 𝑋̄𝑛=^1

#### 𝑛

```
𝑛
∑
𝑖=1
```
#### 𝑋𝑖,

the _sample variance_

#### 𝑆𝑛^2 =

#### 1

#### 𝑛 − 1

```
𝑛
∑
𝑖=1
```
#### (𝑋𝑖−𝑋̄𝑛)^2 ,

and the _sample standard deviation_ or𝑆𝑛=√𝑆𝑛^2.

**Example 0.13.** Suppose𝑋 1 , 𝑋 2 , 𝑋 3 are iid Bernoulli(3/4). What is the sampling distribution
of the sample mean?

```
Table 2:Sampling distribution for for𝑋̄in Example0.13
```
```
Sample Mean Probability
0 1/64
1/3 9/64
2/3 27/64
1 27/64
```
**Exercise 0.10.** Suppose𝑋 1 , 𝑋 2 , ... , 𝑋𝑛are iid Bernoulli(0.3). Find the sampling distribution
of the sample mean for𝑛 = 2and𝑛 = 3.

Once the data are observed we have𝑥 1 , 𝑥 2 , ... , 𝑥𝑛as realizations and the observed values of the
sample, mean, variance, and standard deviation are denoted as ̄𝑥𝑛,𝑠^2 , and𝑠, respectively.

## Expectations


```
Definition 0.17. If𝑋is a random variable and𝑔 ∶ 𝒳 → ℝ, then the expectation of𝑔(𝑋)is if
𝑋is discrete
```
#### 𝐸[𝑔(𝑋)] = ∑

```
𝑥∈𝒳
```
#### 𝑔(𝑥)𝑝(𝑥)

```
and if𝑋is continuous
```
#### 𝐸[𝑔(𝑋)] = ∫

```
𝒳
```
#### 𝑔(𝑥)𝑝(𝑥)𝑑𝑥.

```
In some cases we may need to be explicit about the distribution that is being used in the
expectation and we will write𝐸𝑝[𝑔(𝑋)].
```
```
The notation𝐸[𝑔(𝑋)]is standard in statistics, but in physics the notation⟨𝑔⟩is more
standard. The two notations mean the same thing.
```
**Definition 0.18.** When𝑔(𝑥) = 𝑥𝑘for𝑘 ≥ 1,𝐸[𝑔(𝑋)] = 𝐸[𝑋𝑘]is known as the {𝑘 _th moment}.
The first moment,_ 𝐸[𝑋] _, is the_ mean _of_ 𝑋 _and the_ variance _is defined to be_

```
Var [𝑋] ∶= 𝐸(𝑋 − 𝐸[𝑋])^2 = 𝐸[𝑋^2 ] − (𝐸[𝑋])^2.
```
```
Notice that Var[𝑋] ≥ 0and hence𝐸[𝑋^2 ] ≥ (𝐸[𝑋])^2.
Two common interpretations of𝐸[𝑋]:
```
- 𝐸[𝑋]is the center of mass of a distribution.
- If the experiment is repeated independently, say𝑛times, then the sample mean of these
    realizations

#### 𝑋̄𝑛=^1

#### 𝑛

```
𝑛
∑
𝑖=1
```
#### 𝑋𝑖

```
will approach𝐸[𝑋]as𝑛 → ∞. That is, for suﬀiciently large sample size𝑛,
```
#### 𝑋̄𝑛≈ 𝐸[𝑋].

```
This result is called the law of large numbers.
```

**Definition 0.19.** The _standard deviation_ of a random variable𝑋is defined to be

```
SD[𝑋] =√Var[𝑋].
```
- Notice that Var[𝑋] = 𝐸[𝑋 − 𝐸[𝑋]]^2 ≥ 0. Hence SD[𝑋] ≥ 0.
- SD[𝑋]measures spread of𝑋’s probability distribution.
- 1/SD[𝑋]measures the concentration of probability in a neighborhood of𝐸[𝑋].

**Example 0.14.** The first and second moments of a Bernoulli (𝜃) are

#### 𝐸[𝑋] = ∑

```
𝑥∈{0,1}
```
#### 𝑥𝑝(𝑥) = 0 ⋅ 𝑝(0) + 1 ⋅ 𝑝(1) = 𝑝(1) = 𝜃

and similarly

#### 𝐸[𝑋^2 ] = 𝜃

so that Var[𝑋] = 𝜃(1 − 𝜃).

**Example 0.15.** The first and second moments of a Uniform(0, 1)distribution are

#### 𝐸[𝑋] = ∫

```
1
```
```
0
```
#### 𝑥𝑑𝑥 =

#### 1

#### 2

and

#### 𝐸[𝑋^2 ] = ∫

```
1
```
```
0
```
#### 𝑥^2 𝑑𝑥 =

#### 1

#### 3

so that Var[𝑋] = 1/12.

**Exercise 0.11.** What are the mean and variance of a Uniform(𝛼, 𝛽)where−∞ < 𝛼 < 𝛽 < ∞?

**Exercise 0.12.** Find𝐸[𝑋]and Var[𝑋]if𝑋 ∼Beta(𝛼, 𝛽).


**Exercise 0.13.** Find𝐸[𝑋]and Var[𝑋]if𝑋 ∼Poisson(𝜃).

```
Moments do not always exist.
```
**Example 0.16.** The canonical example where moments do not exist is the standard Cauchy
distribution, denoted Cauchy(0, 1), with probability density function

#### 𝑝(𝑥) =

#### 1

#### 𝜋

#### 1

#### 1 + 𝑥^2

#### 𝐼 (−∞ < 𝑥 < ∞).

In this case, the mean,𝐸[𝑋]does not exist. Note that, in particular, the law of large numbers
does not hold.

```
If𝐸[𝑋𝑘] < ∞, then𝐸[𝑋𝑗] < ∞for all0 ≤ 𝑗 ≤ 𝑘.
```
```
Table 3:Means and variances for some common probability distributions.
```
Name pf Support Mean Variance

Bernoulli(𝜃) 𝜃𝑥(1 − 𝜃)1−𝑥 𝑥 ∈ {0, 1} 𝜃 𝜃(1 − 𝜃)
Binomial(𝑛,𝜃) (𝑛𝑥)𝜃𝑥(1 − 𝜃)𝑛−𝑥 𝑥 ∈ {0, 1, ... , 𝑛} 𝑛𝜃 𝑛𝜃(1 − 𝜃)

Poisson(𝜃) 𝑒
−𝜃𝜃𝑥
𝑥! 𝑥 ∈ {0, 1, 2, ...} 𝜃&𝜃
Uniform(𝛼,𝛽) 𝛽−𝛼^1 𝑥 ∈ [𝛼, 𝛽] 𝛼+𝛽 2 (𝛽−𝛼)

2
12
Beta(𝛼,𝛽) 𝐵(𝛼,𝛽)^1 𝑥𝛼−1(1 −

```
𝑥)𝛽−1
```
#### 𝑥 ∈ [0, 1] 𝛼+𝛽𝛼 (𝛼+𝛽) 2 𝛼𝛽(𝛼+𝛽+1)

Gamma(𝛼,𝛽) 𝛽

```
𝛼
Γ(𝛼)𝑥
```
#### 𝛼−1𝑒−𝛽𝑥 𝑥 ≥ 0 𝛼

```
𝛽
```
𝛼
𝛽^2
Cauchy(0,1)^1 𝜋1+𝑥^12 𝑥 ∈ ℝ DNE DNE

## Parameters and Estimators

**Definition 0.20.** A _parameter_ is a fixed unknown feature of a probability distribution. Pa-
rameters are typically denoted with Greek letters.

A few examples follow. - Suppose𝑋 ∼Bernoulli(𝜃). Here𝜃is a parameter.


- Suppose𝑋 ∼Beta(𝛼, 𝛽). Then both𝛼and𝛽are parameters. If instead𝑋 ∼Beta(2, 𝛽),
    then only𝛽is a parameter.
- The moments of a distribution,𝐸[𝑋𝑘], are parameters.

Statistics are _estimators_ of parameters. If𝑋 1 , 𝑋 2 , ... , 𝑋𝑛is a sample then the sample mean,
𝑋̄𝑛, is an estimator of𝐸[𝑋], the sample variance, 𝑆^2 , is an estimator of Var[𝑋], and the
sample standard deviation,𝑆, is an estimator of SD[𝑋]. The observed values ̄𝑥𝑛,𝑠^2 , and𝑠
are _estimates_. So estimators are random variables and estimates are their observed values in
a particular data set.

```
Of course, even if𝐸[𝑋]does not exist, the sample mean𝑋̄𝑛, for example, can still be
calculated.
```
## Normal Distribution

The most important probability distribution is the Normal distribution, which we now dis-
cuss.

**Definition 0.21.** A continuous random variable𝑋has a _Normal_ distribution with parameters
𝜇and𝜎^2 if it has the pdf

#### 𝑝(𝑥) =

#### 1

#### √

#### 2𝜋𝜎^2

#### 𝑒−

```
2𝜎^12 (𝑥−𝜇)^2
𝑥 ∈ ℝ, 𝜇 ∈ ℝ, 𝜎^2 > 0.
```
Write𝑋|𝜇, 𝜎^2 ∼ 𝑁 (𝜇, 𝜎^2 )or𝑋 ∼ 𝑁 (𝜇, 𝜎^2 ). If𝑋 ∼ 𝑁 (𝜇, 𝜎^2 ), then

- 𝐸[𝑋] = 𝜇,
- Var[𝑋] = 𝜎^2 , and
- 𝑆𝐷[𝑋] = 𝜎.

**Definition 0.22.** A continuous random variable𝑋 has a _standard Normal_ distribution if
𝑋 ∼ 𝑁 (0, 1).

The standard Normal density is displayed in Figure 6.


```
0.0
```
```
0.1
```
```
0.2
```
```
0.3
```
```
0.4
```
```
−4 −2 0 2 4
```
```
y
```
```
Figure 6:The standard normal density.
```
**Example 0.17.** Suppose𝑍 ∼ 𝑁 (0, 1)and we want to calculate𝑃 (𝑎 ≤ 𝑍 ≤ 𝑏). Formally,

#### 𝑃 (𝑎 ≤ 𝑍 ≤ 𝑏) = ∫

```
𝑏
```
```
𝑎
```
#### 1

#### √

#### 2𝜋

#### 𝑒−

```
1
2𝜎^2 (𝑥−𝜇)
2
𝑑𝑥
```
but this can’t be done analytically. We can use R however via the properties of probability
functions that were presented earlier:

#### 𝑃 (𝑎 ≤ 𝑍 ≤ 𝑏) = 𝑃 (𝑍 ≤ 𝑏) − 𝑃 (𝑍 ≤ 𝑎).

```
# P(-1 \le Z \le 1)
pnorm( 1 )- pnorm(- 1 )
```
#### [1] 0.6826895

```
# P(-2 \le Z \le 2)
pnorm( 2 )- pnorm(- 2 )
```

#### [1] 0.9544997

```
# P(-3 \le Z \le 3)
pnorm( 3 )- pnorm(- 3 )
```
#### [1] 0.9973002

```
# P(Z \le 0)
pnorm( 0 )
```
#### [1] 0.5

```
If𝑋 ∼ 𝑁 (𝜇, 𝜎^2 ), then
```
#### 𝑍 =

#### 𝑋 − 𝜇

#### 𝜎

#### ∼ 𝑁 (0, 1).

```
Thus
```
```
𝑃 (𝑎 ≤ 𝑋 ≤ 𝑏) = 𝑃 ((𝑎 − 𝜇)/𝜎 ≤ 𝑍 ≤ (𝑏 − 𝜇)/𝜎)
and we can again usepnormto calculate the desired probability.
```
### Why is the Normal distribution so important?.

Suppose𝑋follows _any_ distribution with mean𝜇and variance𝜎^2. (Note that this does not
include the Cauchy distribution.) Then recall that if an experiment is repeated independently,
say𝑛times, then the law of large numbers says that the sample mean

#### 𝑋̄𝑛=^1

#### 𝑛

```
𝑛
∑
𝑖=1
```
#### 𝑋𝑖

will approach𝜇as𝑛 → ∞. The sample variance

#### 𝑆𝑛^2 =

#### 1

#### 𝑛

```
𝑛
∑
𝑖=1
```
#### (𝑋𝑖−𝑋𝑛)^2

will approach𝜎^2 as𝑛 → ∞and


```
𝑋̄𝑛approx∼ 𝑁 (𝜇, 𝜎^2 /𝑛).
```
The quantity𝜎/

#### √

```
𝑛is known as the standard error for𝑋𝑛.
```
A _little_ more formally, as𝑛 → ∞,

```
√
𝑛(𝑋𝑛− 𝜇)
𝜎
```
#### → 𝑁 (0, 1).

This is the _central limit theorem_ (CLT) and we will revisit it several times in this course.

**Example 0.18.** Suppose𝑋 ∼Bernoulli(.3). The followingRcode will produce 750 indepen-
dent replications of the sample mean based on 1e3 independent Bernoulli(.3)realizations. The
results are given in

```
set.seed( 5731 )
nrep<- 1e3
xbar<- matrix(NA_real_, ncol= 1 , nrow=nrep)
```
```
nsim<- 1e3
theta<-. 3
for(iterin 1 :nrep){
dat <- rbinom(nsim, 1 , theta)
xbar[iter] <- mean(dat)
}
```
```
xbar_df<- data.frame(xbar)
```
```
xbar_plot<- ggplot(xbar_df,aes(x=xbar)) + geom_histogram(aes(y =..density..),
colour = "black", fill = "white")+
stat_function(fun =dnorm,
args = list(mean = mean(xbar_df$xbar), sd = sd(xbar_df$xbar)))
```
**Exercise 0.14.** How many independent Bernoulli trials does it take for the CLT to obtain?
What, if any, impact does the success probability have? What if the distribution is changed
to the Uniform? Or Normal? Or Cauchy? Use simulation to investigate and answer the
questions.


```
0
```
```
10
```
```
20
```
```
30
```
```
0.250 0.275 0.300 0.325
xbar
```
```
density
```
Figure 7:Histogram of 1000 independent replications of the sample mean based on 1000 inde-
pendent Bernoulli(.3)realizations. The solid curve is the normal density centered at
the mean of the replications and with spread equal to the standard deviation of the
replications.


```
Exercise 0.15. The CLT is widely misunderstood. Find the errors in the following quotes?
```
```
The Central Limit Theorem says that the distribution of outcomes is approximately
Normal, after many independent replications of a random experiment, no matter
what the experiment is! (Due to an anonymous tenured Professor of Physics and
Mathematics, but not from UMN!)
```
```
The central limit theorem ensures that the majority of ‘scattered things’ are dis-
persed according to [a Normal distribution]. Wall and Jenkins (2012) (p. 32)
```
## Monte Carlo

The simulation we used to demonstrate the central limit theorem is an example of the _Monte
Carlo method_. Monte Carlo means using a computer to simulate data in order to estimate
parameters of a probability distribution. Monte Carlo will play an important role when we
get to using sophisticated Bayesian models later in the course. Without Monte Carlo, modern
Bayesian methods would be impractical and this course likely would not exist. For now we
will limit attention to some simple examples. Much more is coming later.
Let𝑔 ∶ 𝒳 → ℝ. If a rv has a distribution with pf𝑓, then, in the continuous case,

#### 𝐸[𝑔(𝑋)] = ∫

```
𝒳
```
#### 𝑔(𝑥)𝑓 (𝑥)𝑑𝑥.

Of course, if𝑋 1 , ... , 𝑋𝑚∼ 𝑓, and𝑌𝑖= 𝑔(𝑋𝑖)for𝑖 = 1, ... , 𝑚, then for suﬀiciently large _Monte
Carlo sample size_ ,𝑚,

#### 𝑚̄𝑔 ∶=

#### 1

#### 𝑚

```
𝑚
∑
𝑖=1
```
#### 𝑌𝑖≈ 𝐸[𝑔(𝑋)].

```
This is just another way of expressing the law of large numbers defined earlier. Thus if we
simulate enough realizations, we can use the sample mean to approximate expectations.
```
```
It is important to note that𝑚will be reserved for Monte Carlo sample size, while𝑛will
be used to denote the sample size in the original statistical setting.
```
```
Example 0.19. Consider using Monte Carlo to estimate the mean of the Uniform(0,1) distri-
bution. Of course, we know the mean is 1/2.
```

```
msim<- 50
mc.sims<- runif(msim)
```
```
samp_mean1 <- mean(mc.sims)
```
So with this Monte Carlo sample of size 50 we estimate𝐸[𝑋]to be 0.3649213.

An important point is that another simulation will yield a different result.

```
samp_mean2 <- mean(runif(msim))
```
So the new simulation yields an estimate of 0.4607566. If we repeat this say 50 times (so 50
simulations of length 50) using the code below we obtain Figure 8. In both plots the estimates
are centered around the truth, but when the Monte Carlo sample size is larger, the variability
substantially decreases.

```
0
```
```
1
```
```
2
```
```
3
```
```
4
```
```
5
```
```
0.40 0.45 0.50 0.55 0.60
X1
```
```
count
```
```
0.0
```
```
2.5
```
```
5.0
```
```
7.5
```
```
10.0
```
```
0.40 0.45 0.50 0.55 0.60
X2
```
```
count
```
Figure 8:Histograms of 50 replications of a simulation of lengths 50 and 500. The blue line is
the truth

With the larger Monte Carlo sample size the histogram exhibits the shape of a Normal density,
as it must eventually due to the central limit theorem. This suggests a method for assessing
the reliability of our estimate. In most applications we will not know the truth, so we would
have no idea of how many significant figures to trust in the estimate. If𝑠𝑚is the sample
standard deviation of the𝑌𝑖, the interval defined by

#### 𝑚̄𝑔 ± 2

#### 𝑠𝑚

#### √

#### 𝑚

will describe the reliability of our estimate. Calculating the interval inRis simple. Consider
a simulation


```
msim<- 50
mc.sims<- runif(msim)
m<- mean(mc.sims)
m
```
#### [1] 0.4915452

```
ci <- signif(t.test(mc.sims, conf.level =0.95)$conf.int,digits = 3 )
ci
```
#### [1] 0.409 0.574

attr(,"conf.level")
[1] 0.95

This interval implies that values from 0.409 to 0.574 are plausible and we shouldn’t trust any
of the significant figures in the estimate of 0.49155. Let’s consider a longer simulation.

```
msim<- 500
mc.sims<- runif(msim)
m<- mean(mc.sims)
m
```
#### [1] 0.4818965

```
ci <- signif(t.test(mc.sims, conf.level =0.95)$conf.int,digits = 3 )
ci
```
#### [1] 0.458 0.506

attr(,"conf.level")
[1] 0.95

Indeed after a simulation of length 500 we get a new estimate of 0.4819 and the interval implies
values from 0.458 to 0.506 are plausible and the interval is still too wide to trust any of the
significant figures in the estimate. Let’s consider a longer simulation.


```
msim<- 1e5
mc.sims<- runif(msim)
m<- mean(mc.sims)
m
```
#### [1] 0.5000643

```
ci <- signif(t.test(mc.sims, conf.level =0.95)$conf.int,digits = 3 )
ci
```
#### [1] 0.498 0.502

attr(,"conf.level")
[1] 0.95

Now after a simulation of length 105 we get a new estimate of 0.50006 and the interval implies
values from 0.498 to 0.502 are plausible. Can we stop here? Well it depends on the units of
the problem. If being within 0.002 units (half the length of the interval) is suﬀicient precision,
then we can stop. If not, then not. Keep in mind this took only a fraction of a second to
compute in wall clock time.

**Exercise 0.16.** If𝑋 ∼Gamma(3, 8), use Monte Carlo to estimate𝐸[𝑋] = 3/8and𝑃 (𝑋 ≥
3/8). Try a variety of Monte Carlo sample sizes and calculate the interval estimators to show
the precision of your estimates. Make sure you pay attention to the parameterization of the
Gamma distribution inR.

## Transformations

Recall that a random variable is a real-valued function on the sample space. So if𝑔 is a
real-valued function and𝑋is a random variable, then so is𝑌 = 𝑔(𝑋).

**Theorem 0.3.** _Let_ 𝑋 _be a random variable with density_ 𝑝𝑋(𝑥) _which is continuous on_

#### 𝒳 = {𝑥 ∶ 𝑝𝑋(𝑥) > 0}

_and let_ 𝑌 = 𝑔(𝑋) _, where_ 𝑔 _is a monotone function having a continuous derivative on_


```
𝒴 = {𝑦 ∶ 𝑦 = 𝑔(𝑥) for some 𝑥 ∈ 𝒳}.
```
_Then the density of_ 𝑌 _is_

#### 𝑝𝑌(𝑦) = 𝑝𝑋(𝑔−1(𝑦)) ∣

#### 𝑑

#### 𝑑𝑦

#### 𝑔−1(𝑦)∣.

```
Example 0.20. Suppose𝑋 ∼Gamma(𝛼, 𝛽). What is the distribution of𝑌 = 𝑋−1?
In this case
```
#### ∣

#### 𝑑

#### 𝑑𝑦

#### 𝑔−1(𝑦)∣ =

#### 1

#### 𝑦^2

```
so
```
#### 𝑝𝑌(𝑦) = 𝑝𝑋(1/𝑦)

#### 1

#### 𝑦^2

#### =

#### 𝛽𝛼

#### Γ(𝛼)

#### (1/𝑦)𝛼−1𝑒−𝛽/𝑦

#### 1

#### 𝑦^2

#### =

#### 𝛽𝛼

#### Γ(𝛼)

#### 𝑦−(𝛼+1)𝑒−𝛽/𝑦,

```
which is the density of an Inverse Gamma(𝛼, 𝛽)random variable.
```
```
Exercise 0.17. Show that if𝑋 ∼Inverse Gamma(𝛼, 𝛽), then𝐸(𝑋) = 𝛽/(𝛼 − 1)if𝛼 > 1and
Var(𝑋) = 𝛽^2 /((𝛼 − 1)^2 (𝛼 − 2))if𝛼 > 2.
```
```
Exercise 0.18. Estimate the mean of𝑌 ∼Inverse Gamma(2.5, 3)using Monte Carlo. Simu-
late only Gamma observations usingrgamma. Check your results against the theoretical mean.
```
```
Exercise 0.19. Suppose𝑋 ∼Uniform(0, 1), then what is the density of𝑌 = −𝛽log𝑋for
𝛽 > 0?
```
```
The theory of transformations is much more developed than what has been presented here. The
interested reader is pointed to Casella and Berger (2002) (Ch 2 and 4) for a more thorough
introduction.
```

## Joint, Conditional, and Marginal Distributions

**Example 0.21.** Consider a simple, boring experiment where two coins are tossed indepen-
dently. What is the distribution of outcomes of the coin tosses?

Let𝑋 1 and𝑋 2 correspond to the first and second coins, respectively. Then there are four
outcomes: HH, HT, TH, TT. If coin𝑖has a probability𝜃𝑖of H, then the probability distribution
is given in Table 4.

```
Table 4:Results of tossing two coins
```
```
Outcomes Probabilities
H, H 𝜃 1 𝜃 2
H, T 𝜃 1 (1 − 𝜃 2 )
MISSING
T, T (1 − 𝜃 1 )(1 − 𝜃 2 )
```
This is a joint (bivariate) probability distribution. For example, if we let the event H correspond
to 1 and T to 0, then

```
Pr(𝑋 1 = 1, 𝑋 2 = 1) = 𝜃 1 𝜃 2.
```
That is, it is a probability function for two random variables.

It is utterly common to measure more than one thing in an experiment so to account for
this we’ll require joint probability distributions. We will focus on the bivariate setting, but
everything generalizes in an obvious way.

Consider the bivariate random vector(𝑋, 𝑌 )where𝑋and𝑌 are two random variables. Gen-
erally, either𝑋or𝑌 may be either continuous or discrete and we’ll encounter examples of
this throughout the course. For now we will content ourselves with the setting where both are
continuous or both are discrete.

### Discrete case

When both are discrete,(𝑋, 𝑌 )has a joint pmf

```
𝑝(𝑥, 𝑦) =Pr(𝑋 = 𝑥, 𝑌 = 𝑦)
```

so that

#### 𝑃 ((𝑋, 𝑌 ) ∈ 𝐴) = ∑

```
(𝑥,𝑦)∈𝐴
```
#### 𝑝(𝑥, 𝑦).

Joint pmfs satisfy𝑝(𝑥, 𝑦) ∈ [0, 1]for all(𝑥, 𝑦) ∈ ℝ^2 and

#### ∑

```
(𝑥,𝑦)∈ℝ^2
```
#### 𝑝(𝑥, 𝑦) = 1.

Expectations are calculated as

#### 𝐸[𝑔(𝑋, 𝑌 )] = ∑

```
(𝑥,𝑦)∈ℝ^2
```
#### 𝑔(𝑥, 𝑦)𝑝(𝑥, 𝑦).

**Example 0.22.** Suppose𝒳 = {(0, 0), (1, 0), (0, 1), (1, 1)}and set

#### 𝑝(0, 0) = 𝑝(0, 1) = 1/6 𝑝(1, 0) = 𝑝(1, 1) = 1/3.

What is Pr(𝑋 = 𝑌 )?

#### 𝑝(1, 1) + 𝑝(0, 0) = 1/2.

Now

#### 𝐸[𝑋𝑌 ] = 0 ⋅ 𝑝(0, 0) + 0 ⋅ 𝑝(1, 0) + 0 ⋅ 𝑝(0, 1) + 1 ⋅ 𝑝(1, 1, ) = 1/3.

There are many distributions on this sample space. Here is another one.

**Example 0.23.** Suppose𝒳 = {(0, 0), (1, 0), (0, 1), (1, 1)}and set

#### 𝑝(0, 0) = 1/12, 𝑝(1, 0) = 5/12 𝑝(0, 1) = 𝑝(1, 1) = 1/4.

What is Pr(𝑋 = 𝑌 )?

#### 𝑝(1, 1) + 𝑝(0, 0) = 1/3.


Now

#### 𝐸[𝑋𝑌 ] = 0 ⋅ 𝑝(0, 0) + 0 ⋅ 𝑝(1, 0) + 0 ⋅ 𝑝(0, 1) + 1 ⋅ 𝑝(1, 1) = 1/4.

Joint pmfs have _marginal_ pmfs defined by

#### 𝑝𝑋(𝑥) = ∑

```
𝑦∈ℝ
```
#### 𝑝(𝑥, 𝑦) 𝑝𝑌(𝑦) = ∑

```
𝑥∈ℝ
```
#### 𝑝(𝑥, 𝑦).

::: {#exm-pmf2} This is a continuation of Example0.22. The marginals are given by

#### 𝑝𝑋(0) = 𝑝(0, 0) + 𝑝(0, 1) = 1/3 𝑝𝑋(1) = 𝑝(1, 1) + 𝑝(1, 0) = 1/3

and

#### 𝑝𝑌(0) = 𝑝(0, 0) + 𝑃 (1, 0) = 1/2 𝑝𝑌(1) = 𝑝(0, 1) + 𝑝(1, 1) = 1/2.

#### :::

```
Marginals do not determine the joint distribution unless the random variables are inde-
pendent.
```
**Exercise 0.20.** Check that the pmf given in Example0.23has the same marginals as the pmf
given in Example0.22.

### Continuous case.

When both𝑋and𝑌 are continuous a joint pdf𝑝(𝑥, 𝑦)satisfies

#### 𝑃 ((𝑥, 𝑦) ∈ 𝐴) = ∫

```
𝐴
```
#### 𝑝(𝑥, 𝑦)𝑑𝑥𝑑𝑦

with expectations calculated as

#### 𝐸[𝑔(𝑋, 𝑌 )] = ∫

```
∞
```
```
−∞
```
#### ∫

```
∞
```
```
−∞
```
#### 𝑔(𝑥, 𝑦)𝑝(𝑥, 𝑦)𝑑𝑥𝑑𝑦.

Marginal pdfs are defined as


#### 𝑝𝑋(𝑥) = ∫

```
∞
```
```
−∞
```
#### 𝑝(𝑥, 𝑦)𝑑𝑦 𝑝𝑌(𝑦) = ∫

```
∞
```
```
−∞
```
#### 𝑝(𝑥, 𝑦)𝑑𝑥.

**Example 0.24.** Let𝑎 > 0and consider

#### ℎ(𝑥, 𝑦) = 𝑥 + 𝑎𝑦 0 < 𝑦 < 1, 0 < 𝑥 < 𝑎.

Nowℎis a pdf if and only if𝑎 = 1because (check this as an exercise)

#### ∫

```
1
```
```
0
```
#### ∫

```
𝑎
```
```
0
```
#### ℎ(𝑥, 𝑦)𝑑𝑥𝑑𝑦 = 𝑎^2.

But

#### 𝑝(𝑥, 𝑦) = 𝑎−2ℎ(𝑥, 𝑦)

is a pdf.

**Exercise 0.21.** Find the marginal densities𝑝𝑋and𝑝𝑌 arising from𝑝(𝑥, 𝑦)defined in Exam-
ple0.24. What are the means of the marginals?

**Definition 0.23.** If𝑝𝑌(𝑦) > 0, then the _conditional_ pf for𝑋 ∣ 𝑌 = 𝑦is

#### 𝑝𝑋∣𝑌(𝑥 ∣ 𝑦) =

#### 𝑝(𝑥, 𝑦)

#### 𝑝𝑌(𝑦)

while if𝑃𝑋(𝑥) > 0, then the conditional for𝑌 ∣ 𝑋 = 𝑥is

#### 𝑝𝑌 ∣𝑋(𝑦 ∣ 𝑥) =

#### 𝑝(𝑥, 𝑦)

#### 𝑝𝑋(𝑥)

#### .

**Exercise 0.22.** Verify that the conditionals are both legitimate pfs.


When both exist we have

#### 𝑝(𝑥, 𝑦) = 𝑝𝑌 ∣𝑋(𝑦 ∣ 𝑥)𝑝𝑋(𝑥) = 𝑝𝑋∣𝑌(𝑥 ∣ 𝑦)𝑝𝑌(𝑦)

which can be rewritten as a version of Bayes rule

#### 𝑝𝑌 ∣𝑋(𝑦 ∣ 𝑥) =

#### 𝑝𝑋∣𝑌(𝑥 ∣ 𝑦)𝑝𝑌(𝑦)

#### 𝑝𝑋(𝑥)

#### .

**Exercise 0.23.** Let𝑎 > 0and consider the joint density defined by

#### 𝑝(𝑥, 𝑦) = 𝑎−2(𝑥 + 𝑎𝑦) 0 < 𝑦 < 1, 0 < 𝑥 < 𝑎.

Find the conditional densities and their means.

**Example 0.25.** Suppose𝑋|𝑌 ∼N(𝑌 , 1)and𝑌 ∼N(0, 1). Then the joint density of(𝑋, 𝑌 )is

#### 𝑝(𝑥, 𝑦) = 𝑝𝑋∣𝑌(𝑥 ∣ 𝑦)𝑝𝑌(𝑦) =

#### 1

#### √

#### 2𝜋

#### 𝑒−

(^12) (𝑥−𝑦) 2
Using the properties of joint distributions we can derive other useful distributions. Recall that
if𝑋 ∼Poisson(𝜆), then𝜆 = 𝐸(𝑋) =var(𝑋). In practice, counts often exhibit more variability
than expected under the assumption of a Poisson distribution. This phenomenon is known
as _overdispersion_. The Negative Binomial distribution provides one model which may work
better than the Poisson in the presence of overdispersion.
If𝑋 ∣ 𝑎, 𝜃 ∼NegBin(𝑎, 𝜃)where0 < 𝜃 < 1, then

#### 𝑝(𝑥 ∣ 𝑎, 𝜃) =

#### Γ(𝑎 + 𝑥)

#### 𝑥!Γ(𝑎)

#### 𝜃𝑎(1 − 𝜃)𝑥 𝑥 = 0, 1, 2, 3, ...

In this case,𝐸[𝑋|𝑎, 𝜃] = 𝑎(1 − 𝜃)/𝜃and Var[𝑋|𝑎, 𝜃] = 𝑎(1 − 𝜃)/𝜃^2. Notice that Var[𝑋|𝑎, 𝜃] =
𝐸[𝑋|𝑎, 𝜃]/𝜃and since0 < 𝜃 < 1, the variance is larger than the mean, which is what may
allow it to deal with overdispersion better than the Poisson.

There are many interpretations of the Negative Binomial, but the one that has become standard
is that it is the result of a Poisson-Gamma mixture. Suppose𝑋|𝜆 ∼Poisson(𝜆)and𝜆 ∼
Gamma(𝑎, 𝜃/(1 − 𝜃)). Then


#### 𝑝(𝑥 ∣ 𝜆)𝑝(𝜆)

will define a joint probability function for𝑋, 𝜆so if we integrate with respect to𝜆we will
obtain a marginal pmf for𝑋. We begin with the observation that, by recognizing the integrand
as the kernel of a Gamma density, we have

#### ∫ 𝜆𝑎+𝑥−1𝑒−𝜆/(1−𝜃)𝑑𝜆 = Γ(𝑎 + 𝑥)(1 − 𝜃)𝑎+𝑥.

Then

```
𝑝(𝑥|𝑎, 𝜃) = ∫ 𝑝(𝑥|𝜆)𝑝(𝜆)𝑑𝜆
```
#### = ∫

#### 𝑒−𝜆𝜆𝑥

#### 𝑥!

#### 𝜃𝑎

#### Γ(𝑎)(1 − 𝜃)𝑎

#### 𝜆𝑎−1𝑒−𝜆𝜃/(1−𝜃)𝑑𝜆

#### =

#### 𝜃𝑎

#### 𝑥!Γ(𝑎)(1 − 𝜃)𝑎

#### ∫ 𝜆𝑎+𝑥−1𝑒−𝜆/(1−𝜃)𝑑𝜆

#### =

#### Γ(𝑎 + 𝑥)

#### 𝑥!Γ(𝑎)

#### 𝜃𝑎(1 − 𝜃)

```
𝑥
,
```
which is the Negative Binomial pmf.

## References.

Casella, Georege, and Roger L. Berger. 2002. _Statistical Inference_. 2nd ed. Pacific Grove, CA:
Duxbury.
Cox, R. T. 1946. “Probability, Frequency, and Reasonable Expectation.” _American Journal
of Physics_ 14: 1–13.
Hàjek, Alan. 2020. “Stanford Encyclopedia of Philosophy: Interpretations of Probability.”
https://plato.stanford.edu/entries/probability-interpret/.
Hilbe, Joseph M., Rafael S. de Souza, and Emille E. O. Ishida. 2017. _Bayesian Models for
Astrophysical Data_. Cambridge University Press.
Wall, J. V., and C. R. Jenkins. 2012. _Practical Statistics for Astronomers_. 2nd ed. Cambridge:
Cambridge University Press.
