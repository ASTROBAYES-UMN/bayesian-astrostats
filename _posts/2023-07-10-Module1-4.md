---
layout: post
title: "Introductory Probability and Statistics"
date: 2023-07-10
categories: 5731
tags: test demo pleaseWork
pin: false
---

# Introductory Probability and Statistics
_originally made by Galin Jones_

This tutorial was originally made with R and then adapted to also include the same examples in Python.
If you notice any typos or errors please feel free to email me and I will attempt to fix them as soon as I am able.

Some R packages that will be used to construct parts of these notes.
```
library(tidyverse)
library(ggplot2)
library(modelsummary)
```
## Scope

These notes constitute the briefest introduction to the essentials of probability and statistical
theory useful for our course. Indeed the material presented here is foundational for everything
we will do later, but it is also standard. As such there are already _many_ online resources for
each section.

Most courses on mathematical statistics (e.g. Mathematical Statistics STAT 4101-2, STAT
5101-2, STAT 8101-2) cover probability and statistical theory in vastly more depth and breadth.
MATH 8651-2 (Theory of Probability Including Measure Theory) provides a rigorous course
on probability while MATH 5651-2 provides a more operational treatment.

Introductions to Bayesian methods often spend a fair amount of time on two, of many possible
interpretations of probability; subjectivist versus frequentist. That is not part of the current
program. The interested reader is directed to the Stanford Encyclopedia of Philosophy(HÃ -
jek 2020) for a fuller discussion of the interpretations of probability. Cox (1946) provides a
beautiful classic paper on this topic from a physics point of view.

## Experiments, sample spaces, and events

Definition 0.1. ---
: An _experiment_ is an action or process that generates outcomes. An experi-
ment produces only one outcome.

A few examples.

- Flip a coin once. Observe H or T. (Bernoulli trial)
- Flip the same coin 10 times, independently. Observe the number of H. (Binomial experiment)
- Conduct an election. Observe proportion of votes for each candidate.
- Randomized clinical trial to test a vaccine. Observe how many contract the virus and any potential side effects.
- Citizen science via Zooniverseâ€“Hubble Asteroid Hunter project.
- Examine GPA by major for past students.
- Search to find a galaxy and a quasar within a specified angular distance of each other.
- And so on.

Definition 0.2. ---
: The _sample space_ , denotedğ’³, is the set of all possible outcomes.

Some examples.

- Flip a coin once $\mathcal{X}$ = {ğ», ğ‘‡ }
- Flip a coin twice {% raw %}$\mathcal{X}${% endraw %} = {ğ»ğ», ğ»ğ‘‡ , ğ‘‡ ğ», ğ‘‡ ğ‘‡ }
- Conduct an election. Observe proportion of votes for candidate A. $\mathcal{X}$ = [0, 1]

**Exercise 0.1.** What is the sample space if we flip a coin until the first H? Is it finite?

Definition 0.3. ---
: An _event_ is a set of possible outcomes, that is, any subset of the sample space
and â„± will denote the space of events.

Notice that the empty setâˆ…andğ’³are included in, butâˆ…cannot occur whileğ’³is certain
to occur.

If|, thenâ„±is the power set ofğ’³. More generally, it is ağœ-algebra, but if you are
unfamiliar with measure theory, ignore this.

## Kolmogorov axioms

A real-valued function,ğ‘ƒ, on(ğ’³, â„±)is a _probability function_ (pf) if it satisfies the following
axioms.

1. Ifğ´is an event, then0 â‰¤ ğ‘ƒ (ğ´) â‰¤ 1;
2. ğ‘ƒ (ğ’³) = 1; and
3. Any countable sequence of mutually disjoint eventsğ´ 1 , ğ´ 2 , ...satisfies

#### ğ‘ƒ (âˆªâˆğ‘–=1ğ´ğ‘–) =

```
âˆ
âˆ‘
ğ‘–=
```
#### ğ‘ƒ (ğ´ğ‘–).


```
Theorem 0.1. Let ğ‘ƒ be a probability function and let ğ´ , ğ´ 1 , ... , ğ´ğ‘› and ğµ be events. Then
```
_1._ ğ‘ƒ (âˆ…) = 0 _;_
    _2. For any_ ğ‘› â‰¥ 2 _, if_ ğ´ 1 , ... , ğ´ğ‘› _are mutually disjoint, then_

#### ğ‘ƒ (âˆªğ‘›ğ‘–=1ğ´ğ‘–) =

```
ğ‘›
âˆ‘
ğ‘–=
```
#### ğ‘ƒ (ğ´ğ‘–);

_3. if_ ğ´ âŠ† ğµ _, then_ ğ‘ƒ (ğ´) â‰¤ ğ‘ƒ (ğµ) _;
4._ ğ‘ƒ (ğ´ğ‘) = 1 âˆ’ ğ‘ƒ (ğ´) _; and
5._ ğ‘ƒ (ğ´ âˆª ğµ) = ğ‘ƒ (ğ´) + ğ‘ƒ (ğµ) âˆ’ ğ‘ƒ (ğ´ âˆ© ğµ)_._

```
Exercise 0.2. Prove the theorem using the axioms. Venn diagrams are helpful.
```
## Independence and conditional probability

```
Conditional probability will be foundational to the remainder of the course. Many also find it
counterintuitive at first so it will pay dividends to spend some time on it. The basic idea is that
we want to find the probability of an eventğ´conditional on an eventğµhaving occurred. We
will often say that we want the probability of an eventğ´givenğµ, orğ‘ƒ (ğ´|ğµ), as shorthand.
Letâ€™s begin with an example that may have prompted more arguments than any other because
many find it counterintuitive.
```
```
Example 0.1. Parade magazineâ€™s Ask Marilyn provides a description of the classic Monty
Hall problem: Suppose youâ€™re on a game show, and youâ€™re given the choice of three doors:
Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host,
who knows whatâ€™s behind the doors, opens another door, say No. 3, which has a goat. He then
says to you, â€œDo you want to pick door No. 2?â€ Is it to your advantage to switch your choice?
Before reading any further do, you think it is advantageous to switch? Does your intuition
suggest that it shouldnâ€™t matter?
Suppose you choose Door 2 (Which door you start with doesnâ€™t matter). What is the proba-
bility of winning if you switch? If you donâ€™t switch? From Table 1 we see that there are three
possibilities and you win only one of them if donâ€™t switch while you win two of them if you
switch. Therefore, the probability of winning is doubled if you switch!
```

```
Table 1:Monte Hall Problem
```
```
Door 1 Door 2 Door 3 No Switch Switch
Goat Car Goat W L
Goat Goat Car L W
Car Goat Goat L W
```
**Example 0.2.** Suppose a student wants to apply to a university that admits 75% of applicants
and of those admitted 15% are awarded a scholarship.

Ifğ´denotes the event of being admitted andğµdenotes the event of being awarded a scholar-
ship, then, because a scholarship is awarded only after admission

```
ğ‘ƒ (Awarded scholarship) = ğ‘ƒ (ğµgivenğ´) = ğ‘ƒ (ğµ âˆ£ ğ´) = 0.15.
```
What is the probability that the student is admitted and awarded a scholarship? Well, 15%
of the 75% of those admitted are awarded scholarships so

```
ğ‘ƒ (admission and scholarship) = ğ‘ƒ (ğ´ âˆ© ğµ) = 0.15 Ã— 0.75 = 0.1125.
```
**Definition 0.4.** Letğ´andğµbe events and assumeğ‘ƒ (ğµ) > 0. The _conditional probability_ of
ğ´givenğµis

#### ğ‘ƒ (ğ´ âˆ£ ğµ) =

#### ğ‘ƒ (ğ´ âˆ© ğµ)

#### ğ‘ƒ (ğµ)

#### .

Rearranging we obtainğ‘ƒ (ğ´|ğµ)ğ‘ƒ (ğµ) = ğ‘ƒ (ğ´ âˆ© ğµ). Note that ifğ‘ƒ (ğ´) > 0andğ‘ƒ (ğµ) > 0,
then

#### ğ‘ƒ (ğµ|ğ´) =

#### ğ‘ƒ (ğ´ âˆ© ğµ)

#### ğ‘ƒ (ğ´)

and by rearranging we obtainğ‘ƒ (ğµ|ğ´)ğ‘ƒ (ğ´) = ğ‘ƒ (ğ´ âˆ© ğµ). Hence

#### ğ‘ƒ (ğµ âˆ£ ğ´)ğ‘ƒ (ğ´) = ğ‘ƒ (ğ´ âˆ£ ğµ)ğ‘ƒ (ğµ).

Rearranging this we obtain _Bayes formula_.


**Theorem 0.2.** _If_ ğ‘ƒ (ğ´) > 0 _and_ ğ‘ƒ (ğµ) > 0 _, then_

#### ğ‘ƒ (ğµ âˆ£ ğ´) =

#### ğ‘ƒ (ğ´ âˆ£ ğµ)ğ‘ƒ (ğµ)

#### ğ‘ƒ (ğ´)

#### .

**Exercise 0.3.** Prove the law of total probability

#### ğ‘ƒ (ğ´) = ğ‘ƒ (ğ´ âˆ© ğµ) + ğ‘ƒ (ğ´ âˆ© ğµğ‘)

and obtain an alternative form for Bayes formula

#### ğ‘ƒ (ğµ âˆ£ ğ´) =

#### ğ‘ƒ (ğ´ âˆ£ ğµ)ğ‘ƒ (ğµ)

#### ğ‘ƒ (ğ´ âˆ£ ğµ)ğ‘ƒ (ğµ) + ğ‘ƒ (ğ´ âˆ£ ğµğ‘)ğ‘ƒ (ğµğ‘)

#### .

**Example 0.3.** In the 2020 presidential election there were 276,765 voters in Wyoming and
70% voted for the Republican candidate while in Minnesota there were 3,277,171 voters and
46% voted for the Republican candidate. If a voter is randomly selected from the 3,553,
total voters of the two states, what is the probability that the voter lives in Wyoming if they
voted for the Republican candidate?

Letğ‘Šbe the event that the voter lives in Wyoming andğ‘…the event that they voted for the
Republican candidate. Then we want to findğ‘ƒ (ğ‘Š |ğ‘…). Notice that hereğ‘Šğ‘is the event that
the voter lives in Minnesota and that

#### ğ‘ƒ (ğ‘Š ) =

#### 276, 765

#### 3, 553, 936

#### = 0.078 = 1 âˆ’ ğ‘ƒ (ğ‘Šğ‘).

Using Bayes rule we have

#### ğ‘ƒ (ğ‘Š âˆ£ ğ‘…) =

#### ğ‘ƒ (ğ‘… âˆ£ ğ‘Š )ğ‘ƒ (ğ‘Š )

#### ğ‘ƒ (ğ‘… âˆ£ ğ‘Š )ğ‘ƒ (ğ‘Š ) + ğ‘ƒ (ğ‘… âˆ£ ğ‘Šğ‘)ğ‘ƒ (ğ‘Šğ‘)

#### =

#### 0.7 Ã— 0.

#### 0.7 Ã— 0.078 + 0.46 Ã— 0.

#### = 0.

**Definition 0.5.** The eventsğ´andğµare _independent_ if and only if

#### ğ‘ƒ (ğ´ âˆ© ğµ) = ğ‘ƒ (ğ´)ğ‘ƒ (ğµ).


An alternative and equivalent formulation is that eventsğ´andğµsuch thatğ‘ƒ (ğ´) > 0and
ğ‘ƒ (ğµ) > 0are independent if and only ifğ‘ƒ (ğ´|ğµ) = ğ‘ƒ (ğ´).

Supposeğ´andğµare independent. Then

#### ğ‘ƒ (ğ´ âˆ£ ğµ) =

#### ğ‘ƒ (ğ´ âˆ© ğµ)

#### ğ‘ƒ (ğµ)

#### =

#### ğ‘ƒ (ğ´)ğ‘ƒ (ğµ)

#### ğ‘ƒ (ğµ)

#### = ğ‘ƒ (ğ´).

Supposeğ‘ƒ (ğ´|ğµ) = ğ‘ƒ (ğ´). Then

#### ğ‘ƒ (ğ´) = ğ‘ƒ (ğ´ âˆ£ ğµ) =

#### ğ‘ƒ (ğ´ âˆ© ğµ)

#### ğ‘ƒ (ğµ)

and rearranging we obtainğ‘ƒ (ğ´ âˆ© ğµ) = ğ‘ƒ (ğ´)ğ‘ƒ (ğµ). Henceğ´andğµare independent.

**Exercise 0.4.** Are mutually exclusive events independent?

**Exercise 0.5.** The following statements are equivalent: 1. ğ´andğµare independent. 2. ğ´
andğµğ‘are independent. 3.ğ´ğ‘andğµare independent. 4.ğ´ğ‘andğµğ‘are independent.

## Random variables.

Many (not all!) experiments are conducted so that the outcome generated is a numerical
measurement. It might seem obvious that before an experiment is conducted the outcome is
uncertain and that afterwards there is no uncertainty, but this is a profound distinction.

**Definition 0.6.** A _random variable_ (rv) is a numerical measurement of the outcome of an
experiment that has yet to be performed. More formally, a random variable is a real-valued
function on the sample space.

**Definition 0.7.** A _realization_ of a random variable is the value that the random variable takes
on after an experiment is performed. These are the data.

Upper case letters are reserved for random variables. Soğ‘‹,ğ‘Œ,ğ‘commonly denote random
variables. Lower case letters denote realizations, soğ‘¥,ğ‘¦,ğ‘§.


```
Unique (#) Missing (%) Mean SD Min Median Max
```
```
obsx 43 0 0.0 0. 2 âˆ’ 0. 5 0 .1 0. 3
errx 28 0 0.0 0.0 0. 0 0 .0 0. 1
obsy 41 0 8.4 0.9 6. 5 8 .6 10. 3
erry 25 0 0.1 0.1 0. 0 0 .1 0. 4
```
**Example 0.4.** A supernova is a star that suddenly increases greatly in brightness because of
an explosion that ejects most of its mass. Consider the event of observing a supernova without
assistance over the previous 10 centuries. The random variable of interest is

```
ğ‘‹ =Number of naked-eye supernova observed in previous ten centuries.
```
Notice thatğ‘‹ âˆˆ {0, 1, 2, ...}.

According to Wall and Jenkins (2012), in the 10 centuries before 1987, four naked-eye super-
novae were recorded so the realized value of the random variable isğ‘¥ = 4.

**Example 0.5.** Suppose we are interested in the relationship between the masses of supermas-
sive black holes that lie at the center of a galaxy and the velocity dispersion of the stars in its
bulge. Letğ‘€âŠ™be the solar mass andğ‘€â€¢the mass of the black hole. The natural logarithm^1 of
the black hole mass is taken to beğ‘Œ =log(ğ‘€â€¢/ğ‘€âŠ™)and the natural logarithm of the velocity
dispersion of the stars in its bulge isğ‘‹ =log(ğœğ‘’/ğœ 0 )withğœ 0 a known reference value. Then
bothğ‘‹andğ‘Œ are random variables. The pair(ğ‘‹, ğ‘Œ )is often called a random vector (a vector
of random variables).

Hilbe, de Souza, and Ishida (2017) provide the data summarized below and presented fully
in Figure 1 which consists of 46 observations of the mass of supermassive black holes at the
center of a galaxy and the velocity dispersion of the stars in its bulge. These are the realized
values(ğ‘¥, ğ‘¦)of the random vectors(ğ‘‹, ğ‘Œ ), one for each galaxy.

```
datasummary_skim(bh_dat)
```
```
bh_fig <- ggplot(bh_dat, aes(x=obsx, y=obsy)) + geom_point(size= 1 )+
geom_rug() +
labs(x="Log_10 velocity dispersion",y = "Log_10 black hole mass")
```

```
7
```
```
8
```
```
9
```
```
10
```
```
âˆ’0.50 âˆ’0.25 0.00 0.
Log_10 velocity dispersion
```
```
Log_10 black hole mass
```
```
Figure 1:Plot of log(ğ‘€â€¢/ğ‘€âŠ™)versus log(ğœğ‘’/ğœ 0 ).
```
We will consider two types of random variables, discrete and continuous. Discrete rvs take
on at most countable many values, while continuous rvs take on uncountably many values.
The random variable defined in Example0.4is discrete while the random variables defined in
Example0.5are continuous.

## Probability distributions.

**Definition 0.8.** A _probability distribution_ consists of a random variableâ€™s possible values and
its probability function.

The remainder of this section describes several probability distributions that we will encounter
throughout the course. This section is not intended to be exhaustive, other important distri-
butions will be described or derived later.

(^1) In statistics the logarithm is taken to mean natural logarithm by default. I will try to be explicit about this,
but if the base is unspecified, then reader should use the default.


### Discrete distributions.

The probability function for a discrete random variable is called a _probability mass function_
(pmf). A pmf is a functionğ‘ âˆ¶ ğ’³ â†’ â„such that

#### ğ‘(ğ‘¥) = ğ‘ƒ (ğ‘‹ = ğ‘¥)

```
so thatğ‘(ğ‘¥) â‰¥ 0for allğ‘¥ âˆˆ ğ’³, and
```
#### âˆ‘

```
ğ‘¥âˆˆğ’³
```
#### ğ‘(ğ‘¥) = 1.

```
For a discrete random variable, ifğ´ âˆˆ â„±, then
```
#### ğ‘ƒ (ğ‘‹ âˆˆ ğ´) = âˆ‘

```
ğ‘¥âˆˆğ´
```
#### ğ‘(ğ‘¥).

```
Bernoulli distribution
Recall Example 0.4 where we considered the event of observing naked-eye supernova
over the previous 10 centuries. Letâ€™s consider a simpler setting for now. Define
ğ‘‹ = ğ¼ (at least one supernova observed). Then ğ‘‹ âˆˆ {0, 1}so that if ğœƒ âˆˆ (0, 1) denotes
the probability of observing at least one supernova, then, by Kolmogorovâ€™s axioms,
```
#### ğ‘ƒ (ğ‘‹ = 1) = ğœƒ

```
and
```
#### ğ‘ƒ (ğ‘‹ = 0) = 1 âˆ’ ğ‘ƒ (ğ‘‹ = 1) = 1 âˆ’ ğœƒ.

```
We can write this more concisely as
```
#### ğ‘ƒ (ğ‘‹ = ğ‘¥) = ğœƒğ‘¥(1 âˆ’ ğœƒ)1âˆ’ğ‘¥ ğ‘¥ âˆˆ {0, 1},

```
which is the pmf corresponding to the Bernoulli distribution. This is a useful distribution for
binary random variables. We writeğ‘‹|ğœƒ âˆ¼Bernoulli(ğœƒ)or justğ‘‹ âˆ¼Bernoulli(ğœƒ).
The Bernoulli distribution is a building block for several other common discrete distributions.
We will consider two hereâ€“the Discrete Uniform and the Binomial. We begin with the Discrete
Uniform.
```

**Example 0.6.** Supposeğ‘‹ âˆ¼Bernoulli(1/2). Then

#### ğ‘ƒ (ğ‘‹ = 1) = ğ‘ƒ (ğ‘‹ = 0) = 1/2.

A natural extension is to allowğ‘‹ = 1, 2, ... , ğ‘and set

#### ğ‘ƒ (ğ‘‹ = ğ‘¥) = 1/ğ‘ ğ‘¥ âˆˆ {1, 2, ... , ğ‘ }.

This is the _Discrete Uniform_ distribution and we writeğ‘‹ âˆ¼Discrete Uniform(ğ‘ ).

**Binomial distribution**

**Example 0.7.** Recall Example0.4where we considered the event of observing naked-eye
supernova over the previous 10 centuries. Letâ€™s make the additional assumptions that observing
at least one supernova in each century is independent of which century is chosen and that the
probability of observing at least one supernova in each century,ğœƒ, is constant across centuries.
Then we can define random variablesğ‘Œğ‘– = ğ¼ (at least one supernova observed in centuryğ‘–)
forğ‘– = 1, ... , 10. In this case, the random variablesğ‘Œğ‘–are independent Bernoulli(ğœƒ) random
variables and we can define the total number of centuries in which supernovae were observed
as

#### ğ‘‹ =

```
10
âˆ‘
ğ‘–=
```
#### ğ‘Œğ‘–.

What is the probability distribution ofğ‘‹? We need to derive the probability function. Notice
thatğ‘‹ âˆˆ {0, 1, 2, ... , 10}and that the event thatğ‘‹ = 4, say, can occur in many ways. For
example, it could be the case that

The number of ways of observing 4 distinct centuries with supernovae out of 10 is

#### {ğ‘Œ 1 , ... , ğ‘Œ 10 } = {1, 1, 1, 1, 0, 0, 0, 0, 0, 0}

or

#### {ğ‘Œ 1 , ... , ğ‘Œ 10 } = {1, 0, 1, 1, 1, 0, 0, 0, 0, 0}.

Both of these arrangements have probability (by independence)

#### ğœƒ^4 (1 âˆ’ ğœƒ)^6.


Indeed, by using independence, no matter which order the four centuries with supernovae are
observed this will be the probability. Thus we can find the probability thatğ‘‹ = 4by finding
the number of ways we can observe supernovae in 4 centuries multiplied by probability of any
one way we observe supernovae in 4 centuries or

```
ğ‘ƒ (ğ‘‹ = 4) = [number of ways we can observe supernovae in 4 centuries] Ã— ğœƒ^4 (1 âˆ’ ğœƒ)^6
```
The number of ways of observing 4 distinct centuries with supernovae out of 10 is

#### (

#### 10

#### 4

#### ) =

#### 10!

#### 4!6!

#### = 210.

Therefore,

#### ğ‘ƒ (ğ‘‹ = 4) = 210ğœƒ^4 (1 âˆ’ ğœƒ)^6.

Now the same argument could be made for any of 0 â‰¤ ğ‘‹ â‰¤ 10so we arrive at, forğ‘¥ âˆˆ
{0, 1, 2, ... , 10},

#### ğ‘ƒ (ğ‘‹ = ğ‘¥) = (

#### 10

#### ğ‘¥

#### )ğœƒğ‘¥(1 âˆ’ ğœƒ)10âˆ’ğ‘¥.

**Definition 0.9.** Letğ‘‹be the sum ofğ‘›independent Bernoulli(ğœƒ) trials, then the probability
function ofğ‘‹, forğ‘¥ âˆˆ {0, 1, ... , ğ‘›}and0 â‰¤ ğœƒ â‰¤ 1, is

#### ğ‘ƒ (ğ‘‹ = ğ‘¥) = (

#### ğ‘›

#### ğ‘¥

#### )ğœƒğ‘¥(1 âˆ’ ğœƒ)ğ‘›âˆ’ğ‘¥.

In this caseğ‘‹is a _Binomial_ random variable and we writeğ‘‹|ğœƒ, ğ‘› âˆ¼Binomial(ğ‘›, ğœƒ)or just
ğ‘‹ âˆ¼Binomial(ğ‘›, ğœƒ).

**Example 0.8.** Supposeğ‘‹ âˆ¼Binomial(10, .3). Whenğ‘›is small Binomial probabilities can be
computed directly, but it is easy to use R.

```
# P(X=3)
dbinom( 3 , 10 ,. 3 )
```
#### [1] 0.


```
Unique (#) Missing (%) Mean SD Min Median Max
```
```
N_GC 227 0 1506.1 3949.1 0.0 71.5 32500. 0
MV_T 333 0 âˆ’ 19 .1 2. 8 âˆ’ 24. 2 âˆ’ 19. 6 âˆ’ 11. 2
```
```
# P(X \le 2)
dbinom( 0 , 10 ,. 3 ) + dbinom( 1 , 10 ,. 3 )+dbinom( 2 , 10 ,. 3 )
```
#### [1] 0.

```
#P(X \le 2)
```
```
pbinom( 2 , 10 ,. 3 )
```
#### [1] 0.

**Poisson distribution**

A common setting is to count something which has no natural upper bound, although very
large or very small counts may be unlikely. None of the distributions considered so far are
appropriate for this type of data.

**Example 0.9.** Hilbe, de Souza, and Ishida (2017) provide data of the number of globular
clusters in 420 galaxies. The full data set is plotted in Figure 2 while the galaxies with no
more than 100 globular clusters are plotted in Figure 3.

```
datasummary_skim(NGC_dat)
```
```
NGC_hist<- ggplot(data = NGC_dat,aes(x = N_GC)) +
geom_histogram() +
labs(x = "Number of globular clusters",
y = "Frequency")
```
```
NGC_dat100 <- NGC_dat %>% filter(N_GC<= 100 )
```

```
0
```
```
100
```
```
200
```
```
300
```
```
0 10000 20000 30000
Number of globular clusters
```
```
Frequency
```
```
Figure 2:Frequency plot of globular clusters for all galaxies observed.
```
```
NGC_hist100<- ggplot(data = NGC_dat100,aes(x = N_GC)) +
geom_histogram() +
labs(x = "Number of globular clusters",
y = "Frequency")
```
The Poisson is used a starting point for modelling the distributions of counts. For example, it
can be useful when counting the number of events per unit time or even per unit area. Poisson
processes have even been used to study the nature of coincidences.

**Definition 0.10.** Letğ‘‹ âˆˆ {0, 1, 2, ... , }have pmf

#### ğ‘ƒ (ğ‘‹ = ğ‘¥) =

#### ğ‘’âˆ’ğœƒğœƒğ‘¥

#### ğ‘¥!

#### .

Thenğ‘‹|ğœƒ âˆ¼Poisson(ğœƒ).

The Poisson distribution can be informally derived as a limit from the Binomial or more
formally from the so-called Poisson postulates, but this is beyond our scope.


```
0
```
```
10
```
```
20
```
```
30
```
```
40
```
```
0 25 50 75 100
Number of globular clusters
```
```
Frequency
```
```
Figure 3:Frequency plot of globular clusters for subset of galaxies with no more than 100.
```
### Continuous Distributions

**Definition 0.11.** The probability function for a continuous random variable is called a _prob-
ability density function_ (pdf). A pdf is a functionğ‘ âˆ¶ ğ’³ â†’ â„such thatğ‘(ğ‘¥) â‰¥ 0for allğ‘¥ âˆˆ ğ’³
and

#### âˆ«

```
ğ’³
```
#### ğ‘(ğ‘¥)ğ‘‘ğ‘¥ = 1.

For a continuous random variable, ifğ´ âˆˆ â„±, then

#### ğ‘ƒ (ğ‘‹ âˆˆ ğ´) = âˆ«

```
ğ´
```
#### ğ‘(ğ‘¥)ğ‘‘ğ‘¥.

Notice that for continuous random variablesğ‘ƒ (ğ‘‹ = ğ‘) = 0for anyğ‘while it may be the case
thatğ‘(ğ‘) > 0(points have no width and so do not contribute to the integral).


**Uniform distribution**

Some continuous distributions have discrete analogues. The discrete uniform is characterized
by having a constant probability for each point. Now consider a continuous random variable,
ğ‘‹, on[0, 1]and suppose the probability of the event0 â‰¤ ğ‘ â‰¤ ğ‘‹ â‰¤ ğ‘ â‰¤ 1is constant for
any interval of length ğ‘ âˆ’ ğ‘. This implies the probability is constant on[0, 1], and, since
ğ‘ƒ (0 â‰¤ ğ‘‹ â‰¤ 1) = 1, we have that the probability function isğ‘(ğ‘¥) = ğ¼ (0 â‰¤ ğ‘¥ â‰¤ 1).

**Definition 0.12.** Letğ‘‹be a continuous random variable on[0, 1]with pdfğ‘(ğ‘¥) = ğ¼ (0 â‰¤ ğ‘¥ â‰¤
1), thenğ‘‹is distributed as the _standard Uniform distribution_ and we writeğ‘‹ âˆ¼Uniform(0, 1).

**Exercise 0.6.** How would the Uniform distribution change if the random variable may take on
values between two arbitrary, but finite, real numbersğ›¼ < ğ›½? That is, what is the probability
function ifğ‘‹ âˆ¼Uniform(ğ›¼, ğ›½)?

**Beta distribution**

Supposeğ’³ = [0, 1], but we want to allow more general shapes to the density function than is
possible with the Uniform distribution. For example, it might be more likely to observe the
event0.45 â‰¤ ğ‘‹ â‰¤ 0.55, thanğ‘‹ â‰¥ 0.9, say.

**Example 0.10.** Hilbe, de Souza, and Ishida (2017) provide data for the baryon fraction in
atomic gas for 1175 low-mass galaxies. The baryon fraction takes on values between 0 and 1,
but does not appear to be uniformly distributed on[0, 1]; see Figure 4 for an estimated pdf.

```
bary_dat<- bary_dat %>% mutate(fgas = M_HI /(M_HI+ M_STAR))
```
```
bary_dens<- ggplot(data = bary_dat, aes(x = fgas)) +
geom_density() +
labs(x = "Baryon fraction in atomic gas")
```
The Beta(ğ›¼, ğ›½)distribution, which generalizes the standard Uniform, may be a useful model
for this sort of data.


```
0.
```
```
0.
```
```
1.
```
```
1.
```
```
0.00 0.25 0.50 0.75 1.
Baryon fraction in atomic gas
```
```
density
```
```
Figure 4:Estimated density function of baryon fraction.
```
**Definition 0.13.** Let

#### Î“(ğ‘§) = âˆ«

```
âˆ
```
```
0
```
#### ğ‘¥ğ‘§âˆ’1ğ‘’âˆ’ğ‘¥ğ‘‘ğ‘¥

and

#### ğµ(ğ›¼, ğ›½) =

#### Î“(ğ›¼)Î“(ğ›½)

#### Î“(ğ›¼ + ğ›½)

#### .

Letğ›¼ > 0, ğ›½ > 0. Ifğ‘‹is a continuous random variable onğ’³ = [0, 1]with pdf

#### ğ‘(ğ‘¥|ğ›¼, ğ›½) =

#### 1

#### ğµ(ğ›¼, ğ›½)

#### ğ‘¥ğ›¼âˆ’1(1 âˆ’ ğ‘¥)ğ›½âˆ’1.

thenğ‘‹|ğ›¼, ğ›½ âˆ¼Beta(ğ›¼, ğ›½)or justğ‘‹ âˆ¼Beta(ğ›¼, ğ›½).

Notice thatğ‘‹ âˆ¼Beta(1, 1)is equivalent toğ‘‹ âˆ¼Uniform(0, 1).

```
The gamma function
```

#### Î“(ğ‘§) = âˆ«

```
âˆ
```
```
0
```
#### ğ‘¥ğ‘§âˆ’1ğ‘’âˆ’ğ‘¥ğ‘‘ğ‘¥

```
satisfiesÎ“(ğ‘ + 1) = ğ‘Î“(ğ‘)forğ‘ > 0and, in particular, ifğ‘›is a positive integer, then
Î“(ğ‘›) = (ğ‘› âˆ’ 1)!.
```
**Example 0.11.** Figure 5 displays four Beta densities based on different values for the shape
parameters,ğ›¼andğ›½.

```
0.
```
```
0.
```
```
1.
```
```
1.
```
```
1.
```
```
0.00 0.25 0.50 0.75 1.
shape1=1, shape2=
```
```
y
```
```
0.
```
```
0.
```
```
1.
```
```
1.
```
```
0.00 0.25 0.50 0.75 1.
shape1=1.5, shape2=
```
```
y
```
```
0.
```
```
0.
```
```
1.
```
```
1.
```
```
0.00 0.25 0.50 0.75 1.
shape1=1.5, shape2=
```
```
y
```
```
1
```
```
2
```
```
3
```
```
4
```
```
5
```
```
0.00 0.25 0.50 0.75 1.
shape1=/2, shape2=1/
```
```
y
```
```
Figure 5:Four Beta densities.
```
**Exercise 0.7.** Consider Example0.10. The data can be found at thislink. Read the data
in and construct thefgasfraction. Plot the four beta densities from Example0.11over a
histogram of the constructed fraction. Which most closely agrees with the data?


**Example 0.12.** Beta probabilities can be diï¬€icult to calculate analytically but we can useR.
Supposeğ‘‹ âˆ¼Beta(1.3, 4.1). Then

```
# P(X \ge 0.5)
1 - pbeta(0.5, 1.3,4.1)
```
#### [1] 0.

```
# P(.25 < X \le .5)
pbeta(. 5 , 1.3, 4.1)- pbeta(. 25 , 1.3,4.1)
```
#### [1] 0.

**Gamma distribution**

Just as the Beta distribution allows a variety of shapes on [0, 1], the Gamma distribution
allows a variety onğ’³ = [0, âˆ). We will most commonly encounter the Gamma distribution
as a prior on parameters that must be positive.

**Definition 0.14.** Letğ›¼ > 0, ğ›½ > 0. Ifğ‘‹be a continuous random variable onğ’³ = [0, âˆ)with
pdf

#### ğ‘(ğ‘¥|ğ›¼, ğ›½) =

#### ğ›½ğ›¼

#### Î“(ğ›¼)

#### ğ‘¥ğ›¼âˆ’1ğ‘’âˆ’ğ›½ğ‘¥,

then we sayğ‘‹|ğ›¼, ğ›½ âˆ¼Gamma(ğ›¼, ğ›½). We callğ›¼the shape parameter andğ›½the rate parameter.

The _Exponential distribution_ is a special case of the Gamma distribution whereğ›¼ = 1. If
ğ‘‹ âˆ¼Gamma(1, ğ›½), then

#### ğ‘(ğ‘¥|1, ğ›½) = ğ›½ğ‘’âˆ’ğ›½ğ‘¥.

In this case, we sayğ‘‹|ğ›½ âˆ¼Exponential(ğ›½).

The _chi-squared_ distribution, orğœ’^2 -distribution, results by settingğ›¼ = ğ‘˜/2, withğ‘˜a positive
integer, andğ›½ = 1/2. We sayğ‘‹ âˆ¼ ğœ’^2 ğ‘˜andğ‘˜is called the _degrees of freedom_. The pdf is

#### ğ‘(ğ‘¥|ğ‘˜/2, 1/2) =

#### 1

#### 2 ğ‘˜/2Î“(ğ‘˜/2)

#### ğ‘¥ğ‘˜/2âˆ’1ğ‘’ğ‘¥/2.


```
Another parameterization of the Gamma density is common
```
#### ğ‘(ğ‘¥|ğ›¼, ğ›½) =

#### 1

#### Î“(ğ›¼)ğ›½ğ›¼

#### ğ‘¥ğ›¼âˆ’1ğ‘’âˆ’ğ‘¥/ğ›½.

```
In this parameterizationğ›¼is still called the shape parameter, butğ›½is called the scale
parameter.
The reader should expect either parameterization of the Gamma distribution in software
packages. Which one is used can and will impact the results.
```
**Exercise 0.8.** Plot the Gamma density withğ›¼ = ğ›½ = 0.001,ğ›¼ = ğ›½ = 1,ğ›¼ = 2, ğ›½ = 7.5, and
ğ›¼ = 7.5, ğ‘ğ‘’ğ‘¡ğ‘ = 2.

**Exercise 0.9.** The Poisson and Gamma distributions share many interesting connections.
Supposeğ‘‹ âˆ¼Gamma(ğ›¼, ğ›½)withğ›¼an integer. Then, for anyğ‘¥, ifğ‘Œ âˆ¼Poisson(ğ‘¥/ğ›½),

#### ğ‘ƒ (ğ‘‹ â‰¤ ğ‘¥) = ğ‘ƒ (ğ‘Œ â‰¥ ğ›¼).

Hint: Integration by parts is your friend.

## Statistics

**Definition 0.15.** A collection of random variablesğ‘‹ 1 , ğ‘‹ 2 , ... , ğ‘‹ğ‘›is a _sample_. If the eachğ‘‹ğ‘–
has the same probability distribution and are independent, then we say that is is a _random
sample_ or the sample is _independent and identically distributed_ (iid).

```
The data ,ğ‘¥ 1 , ... , ğ‘¥ğ‘› are realizations of a sampleğ‘‹ 1 , ... , ğ‘‹ğ‘›, but these terms are often
used interchangeably. It is important to keep the distinction in mind.
```
In introductory statistics courses, the sample is often assumed to be iid, mostly for analytical
convenience, and this will indeed be our starting point. However, we will encounter many
examples later in the course where this is an unrealistic assumption and our job will be to
account for the departure.


**Definition 0.16.** A _statistic_ is a real-valued function of the sample. A statistic is itself a
random variable and its probability distribution is called a _sampling distribution_.

Three statistics that we will encounter often are the _sample mean_

#### ğ‘‹Ì„ğ‘›=^1

#### ğ‘›

```
ğ‘›
âˆ‘
ğ‘–=1
```
#### ğ‘‹ğ‘–,

the _sample variance_

#### ğ‘†ğ‘›^2 =

#### 1

#### ğ‘› âˆ’ 1

```
ğ‘›
âˆ‘
ğ‘–=1
```
#### (ğ‘‹ğ‘–âˆ’ğ‘‹Ì„ğ‘›)^2 ,

and the _sample standard deviation_ orğ‘†ğ‘›=âˆšğ‘†ğ‘›^2.

**Example 0.13.** Supposeğ‘‹ 1 , ğ‘‹ 2 , ğ‘‹ 3 are iid Bernoulli(3/4). What is the sampling distribution
of the sample mean?

```
Table 2:Sampling distribution for forğ‘‹Ì„in Example0.13
```
```
Sample Mean Probability
0 1/64
1/3 9/64
2/3 27/64
1 27/64
```
**Exercise 0.10.** Supposeğ‘‹ 1 , ğ‘‹ 2 , ... , ğ‘‹ğ‘›are iid Bernoulli(0.3). Find the sampling distribution
of the sample mean forğ‘› = 2andğ‘› = 3.

Once the data are observed we haveğ‘¥ 1 , ğ‘¥ 2 , ... , ğ‘¥ğ‘›as realizations and the observed values of the
sample, mean, variance, and standard deviation are denoted as Ì„ğ‘¥ğ‘›,ğ‘ ^2 , andğ‘ , respectively.

## Expectations


```
Definition 0.17. Ifğ‘‹is a random variable andğ‘” âˆ¶ ğ’³ â†’ â„, then the expectation ofğ‘”(ğ‘‹)is if
ğ‘‹is discrete
```
#### ğ¸[ğ‘”(ğ‘‹)] = âˆ‘

```
ğ‘¥âˆˆğ’³
```
#### ğ‘”(ğ‘¥)ğ‘(ğ‘¥)

```
and ifğ‘‹is continuous
```
#### ğ¸[ğ‘”(ğ‘‹)] = âˆ«

```
ğ’³
```
#### ğ‘”(ğ‘¥)ğ‘(ğ‘¥)ğ‘‘ğ‘¥.

```
In some cases we may need to be explicit about the distribution that is being used in the
expectation and we will writeğ¸ğ‘[ğ‘”(ğ‘‹)].
```
```
The notationğ¸[ğ‘”(ğ‘‹)]is standard in statistics, but in physics the notationâŸ¨ğ‘”âŸ©is more
standard. The two notations mean the same thing.
```
**Definition 0.18.** Whenğ‘”(ğ‘¥) = ğ‘¥ğ‘˜forğ‘˜ â‰¥ 1,ğ¸[ğ‘”(ğ‘‹)] = ğ¸[ğ‘‹ğ‘˜]is known as the {ğ‘˜ _th moment}.
The first moment,_ ğ¸[ğ‘‹] _, is the_ mean _of_ ğ‘‹ _and the_ variance _is defined to be_

```
Var [ğ‘‹] âˆ¶= ğ¸(ğ‘‹ âˆ’ ğ¸[ğ‘‹])^2 = ğ¸[ğ‘‹^2 ] âˆ’ (ğ¸[ğ‘‹])^2.
```
```
Notice that Var[ğ‘‹] â‰¥ 0and henceğ¸[ğ‘‹^2 ] â‰¥ (ğ¸[ğ‘‹])^2.
Two common interpretations ofğ¸[ğ‘‹]:
```
- ğ¸[ğ‘‹]is the center of mass of a distribution.
- If the experiment is repeated independently, sayğ‘›times, then the sample mean of these
    realizations

#### ğ‘‹Ì„ğ‘›=^1

#### ğ‘›

```
ğ‘›
âˆ‘
ğ‘–=1
```
#### ğ‘‹ğ‘–

```
will approachğ¸[ğ‘‹]asğ‘› â†’ âˆ. That is, for suï¬€iciently large sample sizeğ‘›,
```
#### ğ‘‹Ì„ğ‘›â‰ˆ ğ¸[ğ‘‹].

```
This result is called the law of large numbers.
```

**Definition 0.19.** The _standard deviation_ of a random variableğ‘‹is defined to be

```
SD[ğ‘‹] =âˆšVar[ğ‘‹].
```
- Notice that Var[ğ‘‹] = ğ¸[ğ‘‹ âˆ’ ğ¸[ğ‘‹]]^2 â‰¥ 0. Hence SD[ğ‘‹] â‰¥ 0.
- SD[ğ‘‹]measures spread ofğ‘‹â€™s probability distribution.
- 1/SD[ğ‘‹]measures the concentration of probability in a neighborhood ofğ¸[ğ‘‹].

**Example 0.14.** The first and second moments of a Bernoulli (ğœƒ) are

#### ğ¸[ğ‘‹] = âˆ‘

```
ğ‘¥âˆˆ{0,1}
```
#### ğ‘¥ğ‘(ğ‘¥) = 0 â‹… ğ‘(0) + 1 â‹… ğ‘(1) = ğ‘(1) = ğœƒ

and similarly

#### ğ¸[ğ‘‹^2 ] = ğœƒ

so that Var[ğ‘‹] = ğœƒ(1 âˆ’ ğœƒ).

**Example 0.15.** The first and second moments of a Uniform(0, 1)distribution are

#### ğ¸[ğ‘‹] = âˆ«

```
1
```
```
0
```
#### ğ‘¥ğ‘‘ğ‘¥ =

#### 1

#### 2

and

#### ğ¸[ğ‘‹^2 ] = âˆ«

```
1
```
```
0
```
#### ğ‘¥^2 ğ‘‘ğ‘¥ =

#### 1

#### 3

so that Var[ğ‘‹] = 1/12.

**Exercise 0.11.** What are the mean and variance of a Uniform(ğ›¼, ğ›½)whereâˆ’âˆ < ğ›¼ < ğ›½ < âˆ?

**Exercise 0.12.** Findğ¸[ğ‘‹]and Var[ğ‘‹]ifğ‘‹ âˆ¼Beta(ğ›¼, ğ›½).


**Exercise 0.13.** Findğ¸[ğ‘‹]and Var[ğ‘‹]ifğ‘‹ âˆ¼Poisson(ğœƒ).

```
Moments do not always exist.
```
**Example 0.16.** The canonical example where moments do not exist is the standard Cauchy
distribution, denoted Cauchy(0, 1), with probability density function

#### ğ‘(ğ‘¥) =

#### 1

#### ğœ‹

#### 1

#### 1 + ğ‘¥^2

#### ğ¼ (âˆ’âˆ < ğ‘¥ < âˆ).

In this case, the mean,ğ¸[ğ‘‹]does not exist. Note that, in particular, the law of large numbers
does not hold.

```
Ifğ¸[ğ‘‹ğ‘˜] < âˆ, thenğ¸[ğ‘‹ğ‘—] < âˆfor all0 â‰¤ ğ‘— â‰¤ ğ‘˜.
```
```
Table 3:Means and variances for some common probability distributions.
```
Name pf Support Mean Variance

Bernoulli(ğœƒ) ğœƒğ‘¥(1 âˆ’ ğœƒ)1âˆ’ğ‘¥ ğ‘¥ âˆˆ {0, 1} ğœƒ ğœƒ(1 âˆ’ ğœƒ)
Binomial(ğ‘›,ğœƒ) (ğ‘›ğ‘¥)ğœƒğ‘¥(1 âˆ’ ğœƒ)ğ‘›âˆ’ğ‘¥ ğ‘¥ âˆˆ {0, 1, ... , ğ‘›} ğ‘›ğœƒ ğ‘›ğœƒ(1 âˆ’ ğœƒ)

Poisson(ğœƒ) ğ‘’
âˆ’ğœƒğœƒğ‘¥
ğ‘¥! ğ‘¥ âˆˆ {0, 1, 2, ...} ğœƒ&ğœƒ
Uniform(ğ›¼,ğ›½) ğ›½âˆ’ğ›¼^1 ğ‘¥ âˆˆ [ğ›¼, ğ›½] ğ›¼+ğ›½ 2 (ğ›½âˆ’ğ›¼)

2
12
Beta(ğ›¼,ğ›½) ğµ(ğ›¼,ğ›½)^1 ğ‘¥ğ›¼âˆ’1(1 âˆ’

```
ğ‘¥)ğ›½âˆ’1
```
#### ğ‘¥ âˆˆ [0, 1] ğ›¼+ğ›½ğ›¼ (ğ›¼+ğ›½) 2 ğ›¼ğ›½(ğ›¼+ğ›½+1)

Gamma(ğ›¼,ğ›½) ğ›½

```
ğ›¼
Î“(ğ›¼)ğ‘¥
```
#### ğ›¼âˆ’1ğ‘’âˆ’ğ›½ğ‘¥ ğ‘¥ â‰¥ 0 ğ›¼

```
ğ›½
```
ğ›¼
ğ›½^2
Cauchy(0,1)^1 ğœ‹1+ğ‘¥^12 ğ‘¥ âˆˆ â„ DNE DNE

## Parameters and Estimators

**Definition 0.20.** A _parameter_ is a fixed unknown feature of a probability distribution. Pa-
rameters are typically denoted with Greek letters.

A few examples follow. - Supposeğ‘‹ âˆ¼Bernoulli(ğœƒ). Hereğœƒis a parameter.


- Supposeğ‘‹ âˆ¼Beta(ğ›¼, ğ›½). Then bothğ›¼andğ›½are parameters. If insteadğ‘‹ âˆ¼Beta(2, ğ›½),
    then onlyğ›½is a parameter.
- The moments of a distribution,ğ¸[ğ‘‹ğ‘˜], are parameters.

Statistics are _estimators_ of parameters. Ifğ‘‹ 1 , ğ‘‹ 2 , ... , ğ‘‹ğ‘›is a sample then the sample mean,
ğ‘‹Ì„ğ‘›, is an estimator ofğ¸[ğ‘‹], the sample variance, ğ‘†^2 , is an estimator of Var[ğ‘‹], and the
sample standard deviation,ğ‘†, is an estimator of SD[ğ‘‹]. The observed values Ì„ğ‘¥ğ‘›,ğ‘ ^2 , andğ‘ 
are _estimates_. So estimators are random variables and estimates are their observed values in
a particular data set.

```
Of course, even ifğ¸[ğ‘‹]does not exist, the sample meanğ‘‹Ì„ğ‘›, for example, can still be
calculated.
```
## Normal Distribution

The most important probability distribution is the Normal distribution, which we now dis-
cuss.

**Definition 0.21.** A continuous random variableğ‘‹has a _Normal_ distribution with parameters
ğœ‡andğœ^2 if it has the pdf

#### ğ‘(ğ‘¥) =

#### 1

#### âˆš

#### 2ğœ‹ğœ^2

#### ğ‘’âˆ’

```
2ğœ^12 (ğ‘¥âˆ’ğœ‡)^2
ğ‘¥ âˆˆ â„, ğœ‡ âˆˆ â„, ğœ^2 > 0.
```
Writeğ‘‹|ğœ‡, ğœ^2 âˆ¼ ğ‘ (ğœ‡, ğœ^2 )orğ‘‹ âˆ¼ ğ‘ (ğœ‡, ğœ^2 ). Ifğ‘‹ âˆ¼ ğ‘ (ğœ‡, ğœ^2 ), then

- ğ¸[ğ‘‹] = ğœ‡,
- Var[ğ‘‹] = ğœ^2 , and
- ğ‘†ğ·[ğ‘‹] = ğœ.

**Definition 0.22.** A continuous random variableğ‘‹ has a _standard Normal_ distribution if
ğ‘‹ âˆ¼ ğ‘ (0, 1).

The standard Normal density is displayed in Figure 6.


```
0.0
```
```
0.1
```
```
0.2
```
```
0.3
```
```
0.4
```
```
âˆ’4 âˆ’2 0 2 4
```
```
y
```
```
Figure 6:The standard normal density.
```
**Example 0.17.** Supposeğ‘ âˆ¼ ğ‘ (0, 1)and we want to calculateğ‘ƒ (ğ‘ â‰¤ ğ‘ â‰¤ ğ‘). Formally,

#### ğ‘ƒ (ğ‘ â‰¤ ğ‘ â‰¤ ğ‘) = âˆ«

```
ğ‘
```
```
ğ‘
```
#### 1

#### âˆš

#### 2ğœ‹

#### ğ‘’âˆ’

```
1
2ğœ^2 (ğ‘¥âˆ’ğœ‡)
2
ğ‘‘ğ‘¥
```
but this canâ€™t be done analytically. We can use R however via the properties of probability
functions that were presented earlier:

#### ğ‘ƒ (ğ‘ â‰¤ ğ‘ â‰¤ ğ‘) = ğ‘ƒ (ğ‘ â‰¤ ğ‘) âˆ’ ğ‘ƒ (ğ‘ â‰¤ ğ‘).

```
# P(-1 \le Z \le 1)
pnorm( 1 )- pnorm(- 1 )
```
#### [1] 0.6826895

```
# P(-2 \le Z \le 2)
pnorm( 2 )- pnorm(- 2 )
```

#### [1] 0.9544997

```
# P(-3 \le Z \le 3)
pnorm( 3 )- pnorm(- 3 )
```
#### [1] 0.9973002

```
# P(Z \le 0)
pnorm( 0 )
```
#### [1] 0.5

```
Ifğ‘‹ âˆ¼ ğ‘ (ğœ‡, ğœ^2 ), then
```
#### ğ‘ =

#### ğ‘‹ âˆ’ ğœ‡

#### ğœ

#### âˆ¼ ğ‘ (0, 1).

```
Thus
```
```
ğ‘ƒ (ğ‘ â‰¤ ğ‘‹ â‰¤ ğ‘) = ğ‘ƒ ((ğ‘ âˆ’ ğœ‡)/ğœ â‰¤ ğ‘ â‰¤ (ğ‘ âˆ’ ğœ‡)/ğœ)
and we can again usepnormto calculate the desired probability.
```
### Why is the Normal distribution so important?.

Supposeğ‘‹follows _any_ distribution with meanğœ‡and varianceğœ^2. (Note that this does not
include the Cauchy distribution.) Then recall that if an experiment is repeated independently,
sayğ‘›times, then the law of large numbers says that the sample mean

#### ğ‘‹Ì„ğ‘›=^1

#### ğ‘›

```
ğ‘›
âˆ‘
ğ‘–=1
```
#### ğ‘‹ğ‘–

will approachğœ‡asğ‘› â†’ âˆ. The sample variance

#### ğ‘†ğ‘›^2 =

#### 1

#### ğ‘›

```
ğ‘›
âˆ‘
ğ‘–=1
```
#### (ğ‘‹ğ‘–âˆ’ğ‘‹ğ‘›)^2

will approachğœ^2 asğ‘› â†’ âˆand


```
ğ‘‹Ì„ğ‘›approxâˆ¼ ğ‘ (ğœ‡, ğœ^2 /ğ‘›).
```
The quantityğœ/

#### âˆš

```
ğ‘›is known as the standard error forğ‘‹ğ‘›.
```
A _little_ more formally, asğ‘› â†’ âˆ,

```
âˆš
ğ‘›(ğ‘‹ğ‘›âˆ’ ğœ‡)
ğœ
```
#### â†’ ğ‘ (0, 1).

This is the _central limit theorem_ (CLT) and we will revisit it several times in this course.

**Example 0.18.** Supposeğ‘‹ âˆ¼Bernoulli(.3). The followingRcode will produce 750 indepen-
dent replications of the sample mean based on 1e3 independent Bernoulli(.3)realizations. The
results are given in

```
set.seed( 5731 )
nrep<- 1e3
xbar<- matrix(NA_real_, ncol= 1 , nrow=nrep)
```
```
nsim<- 1e3
theta<-. 3
for(iterin 1 :nrep){
dat <- rbinom(nsim, 1 , theta)
xbar[iter] <- mean(dat)
}
```
```
xbar_df<- data.frame(xbar)
```
```
xbar_plot<- ggplot(xbar_df,aes(x=xbar)) + geom_histogram(aes(y =..density..),
colour = "black", fill = "white")+
stat_function(fun =dnorm,
args = list(mean = mean(xbar_df$xbar), sd = sd(xbar_df$xbar)))
```
**Exercise 0.14.** How many independent Bernoulli trials does it take for the CLT to obtain?
What, if any, impact does the success probability have? What if the distribution is changed
to the Uniform? Or Normal? Or Cauchy? Use simulation to investigate and answer the
questions.


```
0
```
```
10
```
```
20
```
```
30
```
```
0.250 0.275 0.300 0.325
xbar
```
```
density
```
Figure 7:Histogram of 1000 independent replications of the sample mean based on 1000 inde-
pendent Bernoulli(.3)realizations. The solid curve is the normal density centered at
the mean of the replications and with spread equal to the standard deviation of the
replications.


```
Exercise 0.15. The CLT is widely misunderstood. Find the errors in the following quotes?
```
```
The Central Limit Theorem says that the distribution of outcomes is approximately
Normal, after many independent replications of a random experiment, no matter
what the experiment is! (Due to an anonymous tenured Professor of Physics and
Mathematics, but not from UMN!)
```
```
The central limit theorem ensures that the majority of â€˜scattered thingsâ€™ are dis-
persed according to [a Normal distribution]. Wall and Jenkins (2012) (p. 32)
```
## Monte Carlo

The simulation we used to demonstrate the central limit theorem is an example of the _Monte
Carlo method_. Monte Carlo means using a computer to simulate data in order to estimate
parameters of a probability distribution. Monte Carlo will play an important role when we
get to using sophisticated Bayesian models later in the course. Without Monte Carlo, modern
Bayesian methods would be impractical and this course likely would not exist. For now we
will limit attention to some simple examples. Much more is coming later.
Letğ‘” âˆ¶ ğ’³ â†’ â„. If a rv has a distribution with pfğ‘“, then, in the continuous case,

#### ğ¸[ğ‘”(ğ‘‹)] = âˆ«

```
ğ’³
```
#### ğ‘”(ğ‘¥)ğ‘“ (ğ‘¥)ğ‘‘ğ‘¥.

Of course, ifğ‘‹ 1 , ... , ğ‘‹ğ‘šâˆ¼ ğ‘“, andğ‘Œğ‘–= ğ‘”(ğ‘‹ğ‘–)forğ‘– = 1, ... , ğ‘š, then for suï¬€iciently large _Monte
Carlo sample size_ ,ğ‘š,

#### ğ‘šÌ„ğ‘” âˆ¶=

#### 1

#### ğ‘š

```
ğ‘š
âˆ‘
ğ‘–=1
```
#### ğ‘Œğ‘–â‰ˆ ğ¸[ğ‘”(ğ‘‹)].

```
This is just another way of expressing the law of large numbers defined earlier. Thus if we
simulate enough realizations, we can use the sample mean to approximate expectations.
```
```
It is important to note thatğ‘šwill be reserved for Monte Carlo sample size, whileğ‘›will
be used to denote the sample size in the original statistical setting.
```
```
Example 0.19. Consider using Monte Carlo to estimate the mean of the Uniform(0,1) distri-
bution. Of course, we know the mean is 1/2.
```

```
msim<- 50
mc.sims<- runif(msim)
```
```
samp_mean1 <- mean(mc.sims)
```
So with this Monte Carlo sample of size 50 we estimateğ¸[ğ‘‹]to be 0.3649213.

An important point is that another simulation will yield a different result.

```
samp_mean2 <- mean(runif(msim))
```
So the new simulation yields an estimate of 0.4607566. If we repeat this say 50 times (so 50
simulations of length 50) using the code below we obtain Figure 8. In both plots the estimates
are centered around the truth, but when the Monte Carlo sample size is larger, the variability
substantially decreases.

```
0
```
```
1
```
```
2
```
```
3
```
```
4
```
```
5
```
```
0.40 0.45 0.50 0.55 0.60
X1
```
```
count
```
```
0.0
```
```
2.5
```
```
5.0
```
```
7.5
```
```
10.0
```
```
0.40 0.45 0.50 0.55 0.60
X2
```
```
count
```
Figure 8:Histograms of 50 replications of a simulation of lengths 50 and 500. The blue line is
the truth

With the larger Monte Carlo sample size the histogram exhibits the shape of a Normal density,
as it must eventually due to the central limit theorem. This suggests a method for assessing
the reliability of our estimate. In most applications we will not know the truth, so we would
have no idea of how many significant figures to trust in the estimate. Ifğ‘ ğ‘šis the sample
standard deviation of theğ‘Œğ‘–, the interval defined by

#### ğ‘šÌ„ğ‘” Â± 2

#### ğ‘ ğ‘š

#### âˆš

#### ğ‘š

will describe the reliability of our estimate. Calculating the interval inRis simple. Consider
a simulation


```
msim<- 50
mc.sims<- runif(msim)
m<- mean(mc.sims)
m
```
#### [1] 0.4915452

```
ci <- signif(t.test(mc.sims, conf.level =0.95)$conf.int,digits = 3 )
ci
```
#### [1] 0.409 0.574

attr(,"conf.level")
[1] 0.95

This interval implies that values from 0.409 to 0.574 are plausible and we shouldnâ€™t trust any
of the significant figures in the estimate of 0.49155. Letâ€™s consider a longer simulation.

```
msim<- 500
mc.sims<- runif(msim)
m<- mean(mc.sims)
m
```
#### [1] 0.4818965

```
ci <- signif(t.test(mc.sims, conf.level =0.95)$conf.int,digits = 3 )
ci
```
#### [1] 0.458 0.506

attr(,"conf.level")
[1] 0.95

Indeed after a simulation of length 500 we get a new estimate of 0.4819 and the interval implies
values from 0.458 to 0.506 are plausible and the interval is still too wide to trust any of the
significant figures in the estimate. Letâ€™s consider a longer simulation.


```
msim<- 1e5
mc.sims<- runif(msim)
m<- mean(mc.sims)
m
```
#### [1] 0.5000643

```
ci <- signif(t.test(mc.sims, conf.level =0.95)$conf.int,digits = 3 )
ci
```
#### [1] 0.498 0.502

attr(,"conf.level")
[1] 0.95

Now after a simulation of length 105 we get a new estimate of 0.50006 and the interval implies
values from 0.498 to 0.502 are plausible. Can we stop here? Well it depends on the units of
the problem. If being within 0.002 units (half the length of the interval) is suï¬€icient precision,
then we can stop. If not, then not. Keep in mind this took only a fraction of a second to
compute in wall clock time.

**Exercise 0.16.** Ifğ‘‹ âˆ¼Gamma(3, 8), use Monte Carlo to estimateğ¸[ğ‘‹] = 3/8andğ‘ƒ (ğ‘‹ â‰¥
3/8). Try a variety of Monte Carlo sample sizes and calculate the interval estimators to show
the precision of your estimates. Make sure you pay attention to the parameterization of the
Gamma distribution inR.

## Transformations

Recall that a random variable is a real-valued function on the sample space. So ifğ‘” is a
real-valued function andğ‘‹is a random variable, then so isğ‘Œ = ğ‘”(ğ‘‹).

**Theorem 0.3.** _Let_ ğ‘‹ _be a random variable with density_ ğ‘ğ‘‹(ğ‘¥) _which is continuous on_

#### ğ’³ = {ğ‘¥ âˆ¶ ğ‘ğ‘‹(ğ‘¥) > 0}

_and let_ ğ‘Œ = ğ‘”(ğ‘‹) _, where_ ğ‘” _is a monotone function having a continuous derivative on_


```
ğ’´ = {ğ‘¦ âˆ¶ ğ‘¦ = ğ‘”(ğ‘¥) for some ğ‘¥ âˆˆ ğ’³}.
```
_Then the density of_ ğ‘Œ _is_

#### ğ‘ğ‘Œ(ğ‘¦) = ğ‘ğ‘‹(ğ‘”âˆ’1(ğ‘¦)) âˆ£

#### ğ‘‘

#### ğ‘‘ğ‘¦

#### ğ‘”âˆ’1(ğ‘¦)âˆ£.

```
Example 0.20. Supposeğ‘‹ âˆ¼Gamma(ğ›¼, ğ›½). What is the distribution ofğ‘Œ = ğ‘‹âˆ’1?
In this case
```
#### âˆ£

#### ğ‘‘

#### ğ‘‘ğ‘¦

#### ğ‘”âˆ’1(ğ‘¦)âˆ£ =

#### 1

#### ğ‘¦^2

```
so
```
#### ğ‘ğ‘Œ(ğ‘¦) = ğ‘ğ‘‹(1/ğ‘¦)

#### 1

#### ğ‘¦^2

#### =

#### ğ›½ğ›¼

#### Î“(ğ›¼)

#### (1/ğ‘¦)ğ›¼âˆ’1ğ‘’âˆ’ğ›½/ğ‘¦

#### 1

#### ğ‘¦^2

#### =

#### ğ›½ğ›¼

#### Î“(ğ›¼)

#### ğ‘¦âˆ’(ğ›¼+1)ğ‘’âˆ’ğ›½/ğ‘¦,

```
which is the density of an Inverse Gamma(ğ›¼, ğ›½)random variable.
```
```
Exercise 0.17. Show that ifğ‘‹ âˆ¼Inverse Gamma(ğ›¼, ğ›½), thenğ¸(ğ‘‹) = ğ›½/(ğ›¼ âˆ’ 1)ifğ›¼ > 1and
Var(ğ‘‹) = ğ›½^2 /((ğ›¼ âˆ’ 1)^2 (ğ›¼ âˆ’ 2))ifğ›¼ > 2.
```
```
Exercise 0.18. Estimate the mean ofğ‘Œ âˆ¼Inverse Gamma(2.5, 3)using Monte Carlo. Simu-
late only Gamma observations usingrgamma. Check your results against the theoretical mean.
```
```
Exercise 0.19. Supposeğ‘‹ âˆ¼Uniform(0, 1), then what is the density ofğ‘Œ = âˆ’ğ›½logğ‘‹for
ğ›½ > 0?
```
```
The theory of transformations is much more developed than what has been presented here. The
interested reader is pointed to Casella and Berger (2002) (Ch 2 and 4) for a more thorough
introduction.
```

## Joint, Conditional, and Marginal Distributions

**Example 0.21.** Consider a simple, boring experiment where two coins are tossed indepen-
dently. What is the distribution of outcomes of the coin tosses?

Letğ‘‹ 1 andğ‘‹ 2 correspond to the first and second coins, respectively. Then there are four
outcomes: HH, HT, TH, TT. If coinğ‘–has a probabilityğœƒğ‘–of H, then the probability distribution
is given in Table 4.

```
Table 4:Results of tossing two coins
```
```
Outcomes Probabilities
H, H ğœƒ 1 ğœƒ 2
H, T ğœƒ 1 (1 âˆ’ ğœƒ 2 )
MISSING
T, T (1 âˆ’ ğœƒ 1 )(1 âˆ’ ğœƒ 2 )
```
This is a joint (bivariate) probability distribution. For example, if we let the event H correspond
to 1 and T to 0, then

```
Pr(ğ‘‹ 1 = 1, ğ‘‹ 2 = 1) = ğœƒ 1 ğœƒ 2.
```
That is, it is a probability function for two random variables.

It is utterly common to measure more than one thing in an experiment so to account for
this weâ€™ll require joint probability distributions. We will focus on the bivariate setting, but
everything generalizes in an obvious way.

Consider the bivariate random vector(ğ‘‹, ğ‘Œ )whereğ‘‹andğ‘Œ are two random variables. Gen-
erally, eitherğ‘‹orğ‘Œ may be either continuous or discrete and weâ€™ll encounter examples of
this throughout the course. For now we will content ourselves with the setting where both are
continuous or both are discrete.

### Discrete case

When both are discrete,(ğ‘‹, ğ‘Œ )has a joint pmf

```
ğ‘(ğ‘¥, ğ‘¦) =Pr(ğ‘‹ = ğ‘¥, ğ‘Œ = ğ‘¦)
```

so that

#### ğ‘ƒ ((ğ‘‹, ğ‘Œ ) âˆˆ ğ´) = âˆ‘

```
(ğ‘¥,ğ‘¦)âˆˆğ´
```
#### ğ‘(ğ‘¥, ğ‘¦).

Joint pmfs satisfyğ‘(ğ‘¥, ğ‘¦) âˆˆ [0, 1]for all(ğ‘¥, ğ‘¦) âˆˆ â„^2 and

#### âˆ‘

```
(ğ‘¥,ğ‘¦)âˆˆâ„^2
```
#### ğ‘(ğ‘¥, ğ‘¦) = 1.

Expectations are calculated as

#### ğ¸[ğ‘”(ğ‘‹, ğ‘Œ )] = âˆ‘

```
(ğ‘¥,ğ‘¦)âˆˆâ„^2
```
#### ğ‘”(ğ‘¥, ğ‘¦)ğ‘(ğ‘¥, ğ‘¦).

**Example 0.22.** Supposeğ’³ = {(0, 0), (1, 0), (0, 1), (1, 1)}and set

#### ğ‘(0, 0) = ğ‘(0, 1) = 1/6 ğ‘(1, 0) = ğ‘(1, 1) = 1/3.

What is Pr(ğ‘‹ = ğ‘Œ )?

#### ğ‘(1, 1) + ğ‘(0, 0) = 1/2.

Now

#### ğ¸[ğ‘‹ğ‘Œ ] = 0 â‹… ğ‘(0, 0) + 0 â‹… ğ‘(1, 0) + 0 â‹… ğ‘(0, 1) + 1 â‹… ğ‘(1, 1, ) = 1/3.

There are many distributions on this sample space. Here is another one.

**Example 0.23.** Supposeğ’³ = {(0, 0), (1, 0), (0, 1), (1, 1)}and set

#### ğ‘(0, 0) = 1/12, ğ‘(1, 0) = 5/12 ğ‘(0, 1) = ğ‘(1, 1) = 1/4.

What is Pr(ğ‘‹ = ğ‘Œ )?

#### ğ‘(1, 1) + ğ‘(0, 0) = 1/3.


Now

#### ğ¸[ğ‘‹ğ‘Œ ] = 0 â‹… ğ‘(0, 0) + 0 â‹… ğ‘(1, 0) + 0 â‹… ğ‘(0, 1) + 1 â‹… ğ‘(1, 1) = 1/4.

Joint pmfs have _marginal_ pmfs defined by

#### ğ‘ğ‘‹(ğ‘¥) = âˆ‘

```
ğ‘¦âˆˆâ„
```
#### ğ‘(ğ‘¥, ğ‘¦) ğ‘ğ‘Œ(ğ‘¦) = âˆ‘

```
ğ‘¥âˆˆâ„
```
#### ğ‘(ğ‘¥, ğ‘¦).

::: {#exm-pmf2} This is a continuation of Example0.22. The marginals are given by

#### ğ‘ğ‘‹(0) = ğ‘(0, 0) + ğ‘(0, 1) = 1/3 ğ‘ğ‘‹(1) = ğ‘(1, 1) + ğ‘(1, 0) = 1/3

and

#### ğ‘ğ‘Œ(0) = ğ‘(0, 0) + ğ‘ƒ (1, 0) = 1/2 ğ‘ğ‘Œ(1) = ğ‘(0, 1) + ğ‘(1, 1) = 1/2.

#### :::

```
Marginals do not determine the joint distribution unless the random variables are inde-
pendent.
```
**Exercise 0.20.** Check that the pmf given in Example0.23has the same marginals as the pmf
given in Example0.22.

### Continuous case.

When bothğ‘‹andğ‘Œ are continuous a joint pdfğ‘(ğ‘¥, ğ‘¦)satisfies

#### ğ‘ƒ ((ğ‘¥, ğ‘¦) âˆˆ ğ´) = âˆ«

```
ğ´
```
#### ğ‘(ğ‘¥, ğ‘¦)ğ‘‘ğ‘¥ğ‘‘ğ‘¦

with expectations calculated as

#### ğ¸[ğ‘”(ğ‘‹, ğ‘Œ )] = âˆ«

```
âˆ
```
```
âˆ’âˆ
```
#### âˆ«

```
âˆ
```
```
âˆ’âˆ
```
#### ğ‘”(ğ‘¥, ğ‘¦)ğ‘(ğ‘¥, ğ‘¦)ğ‘‘ğ‘¥ğ‘‘ğ‘¦.

Marginal pdfs are defined as


#### ğ‘ğ‘‹(ğ‘¥) = âˆ«

```
âˆ
```
```
âˆ’âˆ
```
#### ğ‘(ğ‘¥, ğ‘¦)ğ‘‘ğ‘¦ ğ‘ğ‘Œ(ğ‘¦) = âˆ«

```
âˆ
```
```
âˆ’âˆ
```
#### ğ‘(ğ‘¥, ğ‘¦)ğ‘‘ğ‘¥.

**Example 0.24.** Letğ‘ > 0and consider

#### â„(ğ‘¥, ğ‘¦) = ğ‘¥ + ğ‘ğ‘¦ 0 < ğ‘¦ < 1, 0 < ğ‘¥ < ğ‘.

Nowâ„is a pdf if and only ifğ‘ = 1because (check this as an exercise)

#### âˆ«

```
1
```
```
0
```
#### âˆ«

```
ğ‘
```
```
0
```
#### â„(ğ‘¥, ğ‘¦)ğ‘‘ğ‘¥ğ‘‘ğ‘¦ = ğ‘^2.

But

#### ğ‘(ğ‘¥, ğ‘¦) = ğ‘âˆ’2â„(ğ‘¥, ğ‘¦)

is a pdf.

**Exercise 0.21.** Find the marginal densitiesğ‘ğ‘‹andğ‘ğ‘Œ arising fromğ‘(ğ‘¥, ğ‘¦)defined in Exam-
ple0.24. What are the means of the marginals?

**Definition 0.23.** Ifğ‘ğ‘Œ(ğ‘¦) > 0, then the _conditional_ pf forğ‘‹ âˆ£ ğ‘Œ = ğ‘¦is

#### ğ‘ğ‘‹âˆ£ğ‘Œ(ğ‘¥ âˆ£ ğ‘¦) =

#### ğ‘(ğ‘¥, ğ‘¦)

#### ğ‘ğ‘Œ(ğ‘¦)

while ifğ‘ƒğ‘‹(ğ‘¥) > 0, then the conditional forğ‘Œ âˆ£ ğ‘‹ = ğ‘¥is

#### ğ‘ğ‘Œ âˆ£ğ‘‹(ğ‘¦ âˆ£ ğ‘¥) =

#### ğ‘(ğ‘¥, ğ‘¦)

#### ğ‘ğ‘‹(ğ‘¥)

#### .

**Exercise 0.22.** Verify that the conditionals are both legitimate pfs.


When both exist we have

#### ğ‘(ğ‘¥, ğ‘¦) = ğ‘ğ‘Œ âˆ£ğ‘‹(ğ‘¦ âˆ£ ğ‘¥)ğ‘ğ‘‹(ğ‘¥) = ğ‘ğ‘‹âˆ£ğ‘Œ(ğ‘¥ âˆ£ ğ‘¦)ğ‘ğ‘Œ(ğ‘¦)

which can be rewritten as a version of Bayes rule

#### ğ‘ğ‘Œ âˆ£ğ‘‹(ğ‘¦ âˆ£ ğ‘¥) =

#### ğ‘ğ‘‹âˆ£ğ‘Œ(ğ‘¥ âˆ£ ğ‘¦)ğ‘ğ‘Œ(ğ‘¦)

#### ğ‘ğ‘‹(ğ‘¥)

#### .

**Exercise 0.23.** Letğ‘ > 0and consider the joint density defined by

#### ğ‘(ğ‘¥, ğ‘¦) = ğ‘âˆ’2(ğ‘¥ + ğ‘ğ‘¦) 0 < ğ‘¦ < 1, 0 < ğ‘¥ < ğ‘.

Find the conditional densities and their means.

**Example 0.25.** Supposeğ‘‹|ğ‘Œ âˆ¼N(ğ‘Œ , 1)andğ‘Œ âˆ¼N(0, 1). Then the joint density of(ğ‘‹, ğ‘Œ )is

#### ğ‘(ğ‘¥, ğ‘¦) = ğ‘ğ‘‹âˆ£ğ‘Œ(ğ‘¥ âˆ£ ğ‘¦)ğ‘ğ‘Œ(ğ‘¦) =

#### 1

#### âˆš

#### 2ğœ‹

#### ğ‘’âˆ’

(^12) (ğ‘¥âˆ’ğ‘¦) 2
Using the properties of joint distributions we can derive other useful distributions. Recall that
ifğ‘‹ âˆ¼Poisson(ğœ†), thenğœ† = ğ¸(ğ‘‹) =var(ğ‘‹). In practice, counts often exhibit more variability
than expected under the assumption of a Poisson distribution. This phenomenon is known
as _overdispersion_. The Negative Binomial distribution provides one model which may work
better than the Poisson in the presence of overdispersion.
Ifğ‘‹ âˆ£ ğ‘, ğœƒ âˆ¼NegBin(ğ‘, ğœƒ)where0 < ğœƒ < 1, then

#### ğ‘(ğ‘¥ âˆ£ ğ‘, ğœƒ) =

#### Î“(ğ‘ + ğ‘¥)

#### ğ‘¥!Î“(ğ‘)

#### ğœƒğ‘(1 âˆ’ ğœƒ)ğ‘¥ ğ‘¥ = 0, 1, 2, 3, ...

In this case,ğ¸[ğ‘‹|ğ‘, ğœƒ] = ğ‘(1 âˆ’ ğœƒ)/ğœƒand Var[ğ‘‹|ğ‘, ğœƒ] = ğ‘(1 âˆ’ ğœƒ)/ğœƒ^2. Notice that Var[ğ‘‹|ğ‘, ğœƒ] =
ğ¸[ğ‘‹|ğ‘, ğœƒ]/ğœƒand since0 < ğœƒ < 1, the variance is larger than the mean, which is what may
allow it to deal with overdispersion better than the Poisson.

There are many interpretations of the Negative Binomial, but the one that has become standard
is that it is the result of a Poisson-Gamma mixture. Supposeğ‘‹|ğœ† âˆ¼Poisson(ğœ†)andğœ† âˆ¼
Gamma(ğ‘, ğœƒ/(1 âˆ’ ğœƒ)). Then


#### ğ‘(ğ‘¥ âˆ£ ğœ†)ğ‘(ğœ†)

will define a joint probability function forğ‘‹, ğœ†so if we integrate with respect toğœ†we will
obtain a marginal pmf forğ‘‹. We begin with the observation that, by recognizing the integrand
as the kernel of a Gamma density, we have

#### âˆ« ğœ†ğ‘+ğ‘¥âˆ’1ğ‘’âˆ’ğœ†/(1âˆ’ğœƒ)ğ‘‘ğœ† = Î“(ğ‘ + ğ‘¥)(1 âˆ’ ğœƒ)ğ‘+ğ‘¥.

Then

```
ğ‘(ğ‘¥|ğ‘, ğœƒ) = âˆ« ğ‘(ğ‘¥|ğœ†)ğ‘(ğœ†)ğ‘‘ğœ†
```
#### = âˆ«

#### ğ‘’âˆ’ğœ†ğœ†ğ‘¥

#### ğ‘¥!

#### ğœƒğ‘

#### Î“(ğ‘)(1 âˆ’ ğœƒ)ğ‘

#### ğœ†ğ‘âˆ’1ğ‘’âˆ’ğœ†ğœƒ/(1âˆ’ğœƒ)ğ‘‘ğœ†

#### =

#### ğœƒğ‘

#### ğ‘¥!Î“(ğ‘)(1 âˆ’ ğœƒ)ğ‘

#### âˆ« ğœ†ğ‘+ğ‘¥âˆ’1ğ‘’âˆ’ğœ†/(1âˆ’ğœƒ)ğ‘‘ğœ†

#### =

#### Î“(ğ‘ + ğ‘¥)

#### ğ‘¥!Î“(ğ‘)

#### ğœƒğ‘(1 âˆ’ ğœƒ)

```
ğ‘¥
,
```
which is the Negative Binomial pmf.

## References.

Casella, Georege, and Roger L. Berger. 2002. _Statistical Inference_. 2nd ed. Pacific Grove, CA:
Duxbury.
Cox, R. T. 1946. â€œProbability, Frequency, and Reasonable Expectation.â€ _American Journal
of Physics_ 14: 1â€“13.
HÃ jek, Alan. 2020. â€œStanford Encyclopedia of Philosophy: Interpretations of Probability.â€
https://plato.stanford.edu/entries/probability-interpret/.
Hilbe, Joseph M., Rafael S. de Souza, and Emille E. O. Ishida. 2017. _Bayesian Models for
Astrophysical Data_. Cambridge University Press.
Wall, J. V., and C. R. Jenkins. 2012. _Practical Statistics for Astronomers_. 2nd ed. Cambridge:
Cambridge University Press.
